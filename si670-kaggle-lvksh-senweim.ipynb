{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing data prepossessing for selecting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 3))\n",
    "\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()\n",
    "X_train_year_onehot = encode.fit_transform(np.array(X_train.iloc[:,2]).reshape(-1,1))\n",
    "X_test_year_onehot = encode.transform(np.array(X_test.iloc[:,2]).reshape(-1,1))\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_year = poly.fit_transform(np.array(X_train.iloc[:,2]).reshape(-1,1))\n",
    "X_test_year  = poly.transform(np.array(X_test.iloc[:,2]).reshape(-1,1))\n",
    "\n",
    "X_train_n_citation = poly.fit_transform(np.array(X_train.iloc[:,4]).reshape(-1,1))\n",
    "X_test_n_citation  = poly.transform(np.array(X_test.iloc[:,4]).reshape(-1,1))\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, \n",
    "                                 X_train_abstract_bigram, \n",
    "                                 X_train_references_bigram, \n",
    "                                 X_train_year_onehot, \n",
    "                                 X_train_year,\n",
    "                                 X_train_n_citation])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, \n",
    "                                 X_test_abstract_bigram, \n",
    "                                 X_test_references_bigram, \n",
    "                                 X_test_year_onehot, \n",
    "                                 X_test_year,\n",
    "                                 X_test_n_citation])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_bigram_numeric_scaled = scaler.fit_transform(X_train_bigram_numeric)\n",
    "X_test_bigram_numeric_scaled  = scaler.transform(X_test_bigram_numeric)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the same to the whole train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train_title_bigram = vectorizer.fit_transform(Data_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(Data_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(Data_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(Data_test.abstract)\n",
    "Data_train.references = Data_train.references.fillna(\"\")\n",
    "Data_test.references = Data_test.references.fillna(\"\")\n",
    "\n",
    "\n",
    "X_train_references_bigram = vectorizer.fit_transform(Data_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(Data_test.references)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()\n",
    "X_train_year_onehot = encode.fit_transform(np.array(Data_train.year).reshape(-1,1))\n",
    "X_test_year_onehot = encode.transform(np.array(Data_test.year).reshape(-1,1))\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_year = poly.fit_transform(np.array(Data_train.year).reshape(-1,1))\n",
    "X_test_year  = poly.transform(np.array(Data_test.year).reshape(-1,1))\n",
    "\n",
    "X_train_n_citation = poly.fit_transform(np.array(Data_train.n_citation).reshape(-1,1))\n",
    "X_test_n_citation  = poly.transform(np.array(Data_test.n_citation).reshape(-1,1))\n",
    "\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "real_X_train_bigram_numeric = hstack([X_train_title_bigram, \n",
    "                                      X_train_abstract_bigram, \n",
    "                                      X_train_references_bigram, \n",
    "                                      X_train_year_onehot,\n",
    "                                      X_train_year,\n",
    "                                      X_train_n_citation])\n",
    "real_X_test_bigram_numeric  = hstack([X_test_title_bigram, \n",
    "                                      X_test_abstract_bigram, \n",
    "                                      X_test_references_bigram, \n",
    "                                      X_test_year_onehot,\n",
    "                                      X_test_year,\n",
    "                                      X_test_n_citation])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "real_X_train_bigram_numeric_scaled = scaler.fit_transform(real_X_train_bigram_numeric)\n",
    "real_X_test_bigram_numeric_scaled  = scaler.transform(real_X_test_bigram_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the selected model onto the whole train dataset\n",
    "I only put the final hyperparameter results here because the hyperparameter tuning process is messy, please contact me if it's anyhow needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for prediction1104_v1_ensemble.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import xgboost as xgb\n",
    "estimators = []\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model4_1 = MLPClassifier(hidden_layer_sizes=(10), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN1', model4_1))\n",
    "model4_2 = MLPClassifier(hidden_layer_sizes=(11), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN2', model4_2))\n",
    "model4_3 = MLPClassifier(hidden_layer_sizes=(20), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN3', model4_3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, n_jobs = -1)\n",
    "\n",
    "ensemble.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "tr = ensemble.score(X_train_bigram_numeric_scaled, y_train)\n",
    "te = ensemble.score(X_test_bigram_numeric_scaled, y_test)\n",
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1104_v1_ensemble.csv\")\n",
    "# The format of the output csv is weird, so I just manually change the column name of this csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for prediction_13NN_ensemble.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import xgboost as xgb\n",
    "estimators = []\n",
    "#model1 = LogisticRegression()\n",
    "#estimators.append(('logistic', model1))\n",
    "# model2 = MultinomialNB()\n",
    "# estimators.append(('MulNB', model2))\n",
    "# model2 = DecisionTreeClassifier()\n",
    "# estimators.append(('cart', model2))\n",
    "# model3 = LinearSVC(C = 0.012345444444444445)\n",
    "# estimators.append(('svm', model3))\n",
    "#model2 = LinearSVC(C = 1)\n",
    "#estimators.append(('svm', model2))\n",
    "#model3 = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        #alpha=1e-2, random_state=42, max_iter=5, tol=None)\n",
    "#estimators.append(('SGD', model3))\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "estimators = []\n",
    "model1 = MLPClassifier(hidden_layer_sizes=(10), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN10', model1))\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(11), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN11', model2))\n",
    "model3 = MLPClassifier(hidden_layer_sizes=(14), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN14', model3))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_10_11_14.csv\")\n",
    "\n",
    "estimators = []\n",
    "model4 = MLPClassifier(hidden_layer_sizes=(15), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN15', model4))\n",
    "model5 = MLPClassifier(hidden_layer_sizes=(16), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN16', model5))\n",
    "model6 = MLPClassifier(hidden_layer_sizes=(18), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN18', model6))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_15_16_18.csv\")\n",
    "\n",
    "estimators = []\n",
    "model7 = MLPClassifier(hidden_layer_sizes=(24), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN24', model7))\n",
    "model8 = MLPClassifier(hidden_layer_sizes=(26), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN26', model8))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_24_26.csv\")\n",
    "\n",
    "estimators = []\n",
    "model9 = MLPClassifier(hidden_layer_sizes=(28), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN28', model9))\n",
    "model10 = MLPClassifier(hidden_layer_sizes=(29), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN29', model10))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_28_29.csv\")\n",
    "\n",
    "estimators = []\n",
    "model11 = MLPClassifier(hidden_layer_sizes=(31), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN31', model11))\n",
    "model12 = MLPClassifier(hidden_layer_sizes=(32), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN32', model12))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_31_32.csv\")\n",
    "\n",
    "\n",
    "model13 = MLPClassifier(hidden_layer_sizes=(33), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN33', model13))\n",
    "model13.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(model13.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_33.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir():\n",
    "    r = re.compile(r'prediction1109')\n",
    "    temp = r.search(file)\n",
    "    if temp is not None:\n",
    "        df['{}'.format(file)] = pd.read_csv(file).iloc[:,1]\n",
    "    \n",
    "output = df.apply(lambda x: stats.mode(x)[0][0], axis = 1)\n",
    "\n",
    "output.name = 'Category'\n",
    "output.to_csv('prediction_13NN_ensemble.csv')\n",
    "# The format of the output csv is weird, so I just manually change the column name of this csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
