{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train_title_bigram = vectorizer.fit_transform(X_train_train.title)\n",
    "X_valid_title_bigram  = vectorizer.transform(X_valid.title)\n",
    "X_train_train_abstract_bigram = vectorizer.fit_transform(X_train_train.abstract)\n",
    "X_valid_abstract_bigram  = vectorizer.transform(X_valid.abstract)\n",
    "X_train_train.references = X_train_train.references.fillna(\"\")\n",
    "X_valid.references = X_valid.references.fillna(\"\")\n",
    "vectorizer_ref = CountVectorizer(decode_error=\"ignore\",ngram_range=(1,1))\n",
    "X_train_train_references_bigram = vectorizer_ref.fit_transform(X_train_train.references)\n",
    "X_valid_references_bigram  = vectorizer_ref.transform(X_valid.references)\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_train_bigram_numeric = hstack([X_train_train_title_bigram, X_train_train_abstract_bigram, X_train_train_references_bigram, X_train_train.iloc[:, [2,4]]])\n",
    "\n",
    "X_valid_bigram_numeric  = hstack([X_valid_title_bigram, X_valid_abstract_bigram, X_valid_references_bigram, X_valid.iloc[:, [2,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores is [0.8359277151547683, 0.8316335659330828, 0.8352120236178208, 0.838074789765611, 0.8378958668813741]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "scores = []\n",
    "n_estimator_list = [300, 400, 500, 600, 1000]\n",
    "for this_n_estimators in n_estimator_list:\n",
    "    rf = RandomForestClassifier(n_jobs=-1, n_estimators = this_n_estimators)\n",
    "    rf.fit(X_train_train_bigram_numeric, y_train_train)\n",
    "    scores.append(rf.score(X_valid_bigram_numeric, y_valid))\n",
    "    \n",
    "\n",
    "print(\"The scores is {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer_ref.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer_ref.transform(X_test.references)\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, X_train.iloc[:, [2,4]]])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, X_test.iloc[:, [2,4]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestID = np.argmax(scores)\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators = n_estimator_list[bestID])\n",
    "rf.fit(X_train_bigram_numeric, y_train)\n",
    "test_score = rf.score(X_test_bigram_numeric, y_test)\n",
    "print(\"Test score is {}\".format(test_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [0.33978048 0.66021952]\n"
     ]
    }
   ],
   "source": [
    "# from adspy_shared_utilities import plot_feature_importances\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10,4), dpi=80)\n",
    "# plot_feature_importances(rf, X_train_bigram_numeric.col)\n",
    "# plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(rf.feature_importances_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score is 0.34174158057158194\n"
     ]
    }
   ],
   "source": [
    "bestID = np.argmax(scores)\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators = n_estimator_list[bestID])\n",
    "rf.fit(X_train.iloc[:,[2,4]], y_train)\n",
    "test_score = rf.score(X_test.iloc[:,[2,4]], y_test)\n",
    "print(\"Test score is {}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, X_train.iloc[:, [2,4]]])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, X_test.iloc[:, [2,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "C_list = np.linspace(1,100, 20)\n",
    "for this_C in C_list:\n",
    "    lr = LogisticRegression(C= this_C, penalty = 'elasticnet', random_state=0, solver = \"saga\", n_jobs=-1,  l1_ratio = 0.5)\n",
    "    lr.fit(X_train_bigram_numeric, y_train)\n",
    "    tr = lr.score(X_train_bigram_numeric, y_train)\n",
    "    te = lr.score(X_test_bigram_numeric, y_test)\n",
    "    print(\"C = {},Training score is {}, testing score is {}\\n\".format(this_C, tr, te))\n",
    "    train_scores.append(tr)\n",
    "    test_scores.append(te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.0001,Training score is 0.7453032742887815, testing score is 0.7304441164631692\n",
      "\n",
      "C = 0.002726315789473684,Training score is 0.9096439434603686, testing score is 0.8561653025627264\n",
      "\n",
      "C = 0.005352631578947368,Training score is 0.9356324923957774, testing score is 0.8647524486783845\n",
      "\n",
      "C = 0.00797894736842105,Training score is 0.9515118983718017, testing score is 0.8690460217362136\n",
      "\n",
      "C = 0.010605263157894736,Training score is 0.9634550008946144, testing score is 0.8709244599490138\n",
      "\n",
      "C = 0.01323157894736842,Training score is 0.9724458758275183, testing score is 0.8709244599490138\n",
      "\n",
      "C = 0.015857894736842102,Training score is 0.9771426015387368, testing score is 0.871863679055414\n",
      "\n",
      "C = 0.018484210526315787,Training score is 0.982778672392199, testing score is 0.8737421172682142\n",
      "\n",
      "C = 0.021110526315789472,Training score is 0.9851046698872786, testing score is 0.8744129880585\n",
      "\n",
      "C = 0.023736842105263157,Training score is 0.9882358203614242, testing score is 0.8746813363746142\n",
      "\n",
      "C = 0.026363157894736842,Training score is 0.9894435498300233, testing score is 0.8746813363746142\n",
      "\n",
      "C = 0.028989473684210523,Training score is 0.9919037394882806, testing score is 0.8757547296390715\n",
      "\n",
      "C = 0.03161578947368421,Training score is 0.994185006262301, testing score is 0.8764256004293574\n",
      "\n",
      "C = 0.03424210526315789,Training score is 0.9913222401145106, testing score is 0.8733395947940427\n",
      "\n",
      "C = 0.03686842105263158,Training score is 0.9944086598675971, testing score is 0.8754863813229572\n",
      "\n",
      "C = 0.03949473684210526,Training score is 0.9950796206834854, testing score is 0.8750838588487857\n",
      "\n",
      "C = 0.04212105263157895,Training score is 0.9966451959205582, testing score is 0.8757547296390715\n",
      "\n",
      "C = 0.04474736842105263,Training score is 0.998389694041868, testing score is 0.8774989936938146\n",
      "\n",
      "C = 0.04737368421052632,Training score is 0.9987028090892826, testing score is 0.8776331678518717\n",
      "\n",
      "C = 0.05,Training score is 0.997405618178565, testing score is 0.8768281229035287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "C_list = np.linspace(0.0001, 0.05, 20)\n",
    "for this_C in C_list:\n",
    "    lr = LogisticRegression(C= this_C, penalty = 'l2', random_state=0)\n",
    "    lr.fit(X_train_bigram_numeric, y_train)\n",
    "    tr = lr.score(X_train_bigram_numeric, y_train)\n",
    "    te = lr.score(X_test_bigram_numeric, y_test)\n",
    "    print(\"C = {},Training score is {}, testing score is {}\\n\".format(this_C, tr, te))\n",
    "    train_scores.append(tr)\n",
    "    test_scores.append(te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014859157825612312,\n",
       " 0.053478640897642205,\n",
       " 0.07088004371739287,\n",
       " 0.08246587663558813,\n",
       " 0.09253054094560065,\n",
       " 0.1015214158785045,\n",
       " 0.10527892248332282,\n",
       " 0.1090365551239848,\n",
       " 0.11069168182877864,\n",
       " 0.11355448398681,\n",
       " 0.11476221345540905,\n",
       " 0.11614900984920906,\n",
       " 0.11775940583294364,\n",
       " 0.11798264532046798,\n",
       " 0.11892227854463988,\n",
       " 0.11999576183469973,\n",
       " 0.1208904662814867,\n",
       " 0.12089070034805338,\n",
       " 0.12106964123741082,\n",
       " 0.12057749527503625]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i - j for i,j in zip(train_scores, test_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=2e-06, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, max_features=0.000002, n_estimators=500)\n",
    "rf.fit(X_train_bigram_numeric, y_train)\n",
    "rf.feature_impotance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fI = rf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fI = pd.DataFrame(fI)\n",
    "fI['INDEX'] = fI.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_index = fI.sort_values(by = [0],ascending = False).INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trunc in module scipy.sparse.data:\n",
      "\n",
      "trunc(self)\n",
      "    Element-wise trunc.\n",
      "    \n",
      "    See numpy.trunc for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(coo_matrix.trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram_numeric_csr = X_train_bigram_numeric.tocsr()\n",
    "X_test_bigram_numeric_csr = X_test_bigram_numeric.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram_numeric_selected = hstack([X_train_bigram_numeric_csr.getcol(i) for i in selected_feature_index[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_bigram_numeric_selected = hstack([X_test_bigram_numeric_csr.getcol(i) for i in selected_feature_index[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max features = 3, train score = 1.0, test score = 0.8212800214678653\n",
      "\n",
      "max features = 4, train score = 1.0, test score = 0.8274520327384945\n",
      "\n",
      "max features = 5, train score = 1.0, test score = 0.8295988192674091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "max_features_list = [3,4,5  ]\n",
    "for this_max_features in max_features_list:\n",
    "    rf = RandomForestClassifier(n_jobs=-1, max_features=this_max_features, n_estimators=500)\n",
    "    rf.fit(X_train_bigram_numeric_selected, y_train)\n",
    "    tr = rf.score(X_train_bigram_numeric_selected, y_train)\n",
    "    te = rf.score(X_test_bigram_numeric_selected, y_test)\n",
    "    print(\"max features = {}, train score = {}, test score = {}\\n\".format(this_max_features,tr,te))\n",
    "    train_scores.append(tr)\n",
    "    test_scores.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.394661"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1394661 * 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 2, train score = 0.870101986044015, test score = 0.8376492687508386\n",
      "\n",
      "C = 5, train score = 0.8721595992127393, test score = 0.8372467462766671\n",
      "\n",
      "C = 10, train score = 0.8730094829128645, test score = 0.8352341339058097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "C_list = [2,5,10]\n",
    "for this_C in C_list:\n",
    "    lr = LogisticRegression(C= this_C, penalty = 'l2', random_state=0)\n",
    "    lr.fit(X_train_bigram_numeric_selected, y_train)\n",
    "    train_score = lr.score(X_train_bigram_numeric_selected, y_train)\n",
    "    test_score = lr.score(X_test_bigram_numeric_selected, y_test)\n",
    "    print(\"C = {}, train score = {}, test score = {}\\n\".format(this_C,train_score,test_score))\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train_title_bigram = vectorizer.fit_transform(Data_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(Data_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(Data_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(Data_test.abstract)\n",
    "Data_train.references = Data_train.references.fillna(\"\")\n",
    "Data_test.references = Data_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(Data_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(Data_test.references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "real_X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, Data_train.iloc[:, [3,5]]])\n",
    "real_X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, Data_test.iloc[:, [3,5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lvkunsheng/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C= 1000, penalty = 'l1')\n",
    "lr.fit(real_X_train_bigram_numeric, Data_train.iloc[:,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(real_X_train_bigram_numeric, Data_train.iloc[:,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(lr.predict(real_X_test_bigram_numeric), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1029_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to get some new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 3))\n",
    "\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\")\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, X_train.iloc[:, [2,4]]])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, X_test.iloc[:, [2,4]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1,Training score is 0.7139470388262659, testing score is 0.6547698913189319\n",
      "\n",
      "C = 0.6210526315789474,Training score is 0.7368938987296475, testing score is 0.6850932510398497\n",
      "\n",
      "C = 1.142105263157895,Training score is 0.7368491680085884, testing score is 0.6850932510398497\n",
      "\n",
      "C = 1.6631578947368424,Training score is 0.7368491680085884, testing score is 0.6850932510398497\n",
      "\n",
      "C = 2.18421052631579,Training score is 0.7368491680085884, testing score is 0.6850932510398497\n",
      "\n",
      "C = 2.7052631578947373,Training score is 0.7368044372875291, testing score is 0.6850932510398497\n",
      "\n",
      "C = 3.2263157894736847,Training score is 0.7368044372875291, testing score is 0.6850932510398497\n",
      "\n",
      "C = 3.747368421052632,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 4.268421052631579,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 4.7894736842105265,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 5.310526315789474,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 5.831578947368421,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 6.352631578947369,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 6.873684210526316,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 7.394736842105264,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 7.915789473684211,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 8.436842105263159,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 8.957894736842105,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 9.478947368421053,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n",
      "C = 10.0,Training score is 0.7367597065664698, testing score is 0.6850932510398497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "C_list = np.linspace(0.1, 10, 20)\n",
    "for this_C in C_list:\n",
    "    lr = LogisticRegression(C= this_C, penalty = 'l2', random_state=0)\n",
    "    lr.fit(X_train_bigram_numeric, y_train)\n",
    "    tr = lr.score(X_train_bigram_numeric, y_train)\n",
    "    te = lr.score(X_test_bigram_numeric, y_test)\n",
    "    print(\"C = {},Training score is {}, testing score is {}\\n\".format(this_C, tr, te))\n",
    "    train_scores.append(tr)\n",
    "    test_scores.append(te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, max_features=this_max_features, n_estimators=200, random_state = 0)\n",
    "rf.fit(X_train_bigram_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max features = 28,train score = 0.7400697799248523, test score = 0.38964175499798737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "test_scores = {}\n",
    "train_scores = {}\n",
    "max_features_list = list(range(3,30,5))\n",
    "# max_depth_list = list(range(50,75,5))\n",
    "\n",
    "#for this_max_features in max_features_list:\n",
    "rf = RandomForestClassifier(n_jobs=-1, max_features=3, max_depth= 1000,n_estimators=200, random_state = 0)\n",
    "rf.fit(X_train_bigram_numeric, y_train)\n",
    "tr = rf.score(X_train_bigram_numeric, y_train)\n",
    "te = rf.score(X_test_bigram_numeric, y_test)\n",
    "print(\"max features = {},train score = {}, test score = {}\\n\".format(this_max_features,tr,te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "C_list = list(range(1,20,2))\n",
    "for this_C in C_list:\n",
    "    svc = SVC(kernel=\"poly\", C = this_C, gamma = \"scale\")\n",
    "    svc.fit(X_train_bigram_numeric, y_train)\n",
    "    tr = svc.score(X_train_bigram_numeric, y_train)\n",
    "    te = svc.score(X_test_bigram_numeric, y_test)\n",
    "    print(\"C = {}, train score = {}, test score = {}\\n\".format(this_C,tr,te))\n",
    "    train_scores.append(tr)\n",
    "    test_scores.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "C_list = list(range(1,20,2))\n",
    "for this_C in C_list:\n",
    "    svc = SVC(kernel=\"linear\", C = this_C, gamma = \"scale\")\n",
    "    svc.fit(X_train_bigram_numeric, y_train)\n",
    "    tr = svc.score(X_train_bigram_numeric, y_train)\n",
    "    te = svc.score(X_test_bigram_numeric, y_test)\n",
    "    print(\"C = {}, train score = {}, test score = {}\\n\".format(this_C,tr,te))\n",
    "    train_scores.append(tr)\n",
    "    test_scores.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 3))\n",
    "X_train_title_bigram = vectorizer.fit_transform(Data_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(Data_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(Data_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(Data_test.abstract)\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\")\n",
    "Data_train.references = Data_train.references.fillna(\"\")\n",
    "Data_test.references = Data_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(Data_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(Data_test.references)\n",
    "from scipy.sparse import hstack\n",
    "real_X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, Data_train.iloc[:, [3,5]]])\n",
    "real_X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, Data_test.iloc[:, [3,5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=100, random_state=0)\n",
    "clf.fit(X_train_bigram_numeric, y_train)\n",
    "tr = clf.score(X_train_bigram_numeric, y_train)\n",
    "te = clf.score(X_test_bigram_numeric, y_test)\n",
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trash tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\")\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, X_train.iloc[:, [2,4]]])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, X_test.iloc[:, [2,4]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score = 0.8445160135981392, test score = 0.8139004427747216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=200, random_state=0)\n",
    "clf.fit(X_train_bigram_numeric, y_train)\n",
    "tr = clf.score(X_train_bigram_numeric, y_train)\n",
    "te = clf.score(X_test_bigram_numeric, y_test)\n",
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-00990e6231ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_bigram_numeric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(X_train_bigram_numeric, y_train)\n",
    "model.score(X_test_bigram_numeric,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': <scipy.stats._distn_infrastructure.rv_frozen at 0x22a4345a6d8>,\n",
       " 'gamma': <scipy.stats._distn_infrastructure.rv_frozen at 0x22a4345ac50>,\n",
       " 'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen at 0x22a4345add8>,\n",
       " 'max_depth': <scipy.stats._distn_infrastructure.rv_frozen at 0x22a4345af60>,\n",
       " 'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen at 0x22a4345ada0>,\n",
       " 'subsample': <scipy.stats._distn_infrastructure.rv_frozen at 0x22a434230b8>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "params = {\n",
    "     \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "     \"gamma\": uniform(0, 0.5),\n",
    "     \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "     \"max_depth\": randint(2, 6), # default 3\n",
    "     \"n_estimators\": randint(100, 150), # default 100\n",
    "     \"subsample\": uniform(0.6, 0.4)\n",
    "    }\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params = {'colsample_bytree': 0.5649476632289705, 'gamma': 0.2542327337891345, 'learning_rate': 0.23418998463693103, 'max_depth': 4, 'n_estimators': 112, 'subsample': 0.5122059514323735}, train score = 0.9068259080336375, test score = 0.8603247014624983\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6940527346556096, 'gamma': 0.48141981866791034, 'learning_rate': 0.14129983892119768, 'max_depth': 4, 'n_estimators': 108, 'subsample': 0.5275992768833908}, train score = 0.8776614779030238, test score = 0.8505299879243258\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3308773376543079, 'gamma': 0.24262554278833465, 'learning_rate': 0.14904783635568786, 'max_depth': 5, 'n_estimators': 149, 'subsample': 0.43760200008140593}, train score = 0.9130434782608695, test score = 0.8644841003622702\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3343265451473223, 'gamma': 0.2010554018708826, 'learning_rate': 0.2723581353036539, 'max_depth': 2, 'n_estimators': 146, 'subsample': 0.49848749761022787}, train score = 0.8706387546967257, test score = 0.8534818194015833\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5397966039122385, 'gamma': 0.3027060470233051, 'learning_rate': 0.20594944533004447, 'max_depth': 4, 'n_estimators': 141, 'subsample': 0.49933217280896547}, train score = 0.9125514403292181, test score = 0.8635448812558701\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.44437176407684625, 'gamma': 0.22425510192910952, 'learning_rate': 0.10465238990227313, 'max_depth': 3, 'n_estimators': 142, 'subsample': 0.4926770851295214}, train score = 0.8531043120415102, test score = 0.8361733530122099\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4507914255646101, 'gamma': 0.1341488183089392, 'learning_rate': 0.2989277215802858, 'max_depth': 4, 'n_estimators': 104, 'subsample': 0.5947374259275359}, train score = 0.9142064770084094, test score = 0.860861398094727\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4708981832012351, 'gamma': 0.2988131033206023, 'learning_rate': 0.169077036189136, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5446511935203268}, train score = 0.9030237967436036, test score = 0.8577753924594124\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.47616501397175337, 'gamma': 0.35051241928770294, 'learning_rate': 0.2963648997570669, 'max_depth': 4, 'n_estimators': 102, 'subsample': 0.46684651255928056}, train score = 0.91071748076579, test score = 0.8618006172011271\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6741601773191345, 'gamma': 0.17441836848260106, 'learning_rate': 0.03697570477856117, 'max_depth': 3, 'n_estimators': 113, 'subsample': 0.5062923017553078}, train score = 0.7873054213633924, test score = 0.7815644706829464\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6120115552783058, 'gamma': 0.3800219657897174, 'learning_rate': 0.03826982695584151, 'max_depth': 5, 'n_estimators': 132, 'subsample': 0.5805576037177123}, train score = 0.8380300590445517, test score = 0.8157788809875218\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6327967048981318, 'gamma': 0.058177737986462585, 'learning_rate': 0.10444604871566006, 'max_depth': 3, 'n_estimators': 124, 'subsample': 0.45100144711123846}, train score = 0.848675970656647, test score = 0.8338923923252382\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.49186123885567923, 'gamma': 0.44222388852446104, 'learning_rate': 0.14639605536251582, 'max_depth': 2, 'n_estimators': 126, 'subsample': 0.530156207418838}, train score = 0.8386115584183217, test score = 0.8295988192674091\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.554810688963285, 'gamma': 0.15474689592255303, 'learning_rate': 0.19164144667637023, 'max_depth': 5, 'n_estimators': 116, 'subsample': 0.5622615844635289}, train score = 0.9212739309357667, test score = 0.8599221789883269\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.31156620388446393, 'gamma': 0.46486251838780773, 'learning_rate': 0.07546875226136862, 'max_depth': 4, 'n_estimators': 142, 'subsample': 0.43775931070075236}, train score = 0.8579352299159062, test score = 0.8389910103314102\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6574588038718939, 'gamma': 0.06374803152749686, 'learning_rate': 0.29501124004898793, 'max_depth': 2, 'n_estimators': 139, 'subsample': 0.4960461556888091}, train score = 0.8747092503131151, test score = 0.855762780088555\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5163572533689792, 'gamma': 0.2590790408439511, 'learning_rate': 0.15738020270387512, 'max_depth': 3, 'n_estimators': 145, 'subsample': 0.4702986827898762}, train score = 0.8736357130076937, test score = 0.8494565946598686\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4552070253543344, 'gamma': 0.4239041600170845, 'learning_rate': 0.27880253945864286, 'max_depth': 4, 'n_estimators': 131, 'subsample': 0.4324425842662615}, train score = 0.91993200930399, test score = 0.8640815778880988\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6754624456467675, 'gamma': 0.1350730350855407, 'learning_rate': 0.2180586574987509, 'max_depth': 5, 'n_estimators': 139, 'subsample': 0.4945994526073451}, train score = 0.9379137591697978, test score = 0.8693143700523279\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.42931408011347527, 'gamma': 0.43199116001404303, 'learning_rate': 0.2842843581011213, 'max_depth': 3, 'n_estimators': 132, 'subsample': 0.5556400543139877}, train score = 0.8988191089640365, test score = 0.86260566214947\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5470916434431123, 'gamma': 0.3031255848563441, 'learning_rate': 0.05402926909974887, 'max_depth': 4, 'n_estimators': 101, 'subsample': 0.46694968234645606}, train score = 0.8218375380211129, test score = 0.8106802629813498\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4282400044536674, 'gamma': 0.40566806774229874, 'learning_rate': 0.10261624046294139, 'max_depth': 3, 'n_estimators': 135, 'subsample': 0.49073756803282004}, train score = 0.8515387368044373, test score = 0.8333556956930095\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4106268346813441, 'gamma': 0.0823198894229118, 'learning_rate': 0.06850460156717661, 'max_depth': 5, 'n_estimators': 119, 'subsample': 0.44399490754791254}, train score = 0.8635265700483091, test score = 0.8377834429088957\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6293028769045879, 'gamma': 0.2461737159639079, 'learning_rate': 0.2962873728833355, 'max_depth': 5, 'n_estimators': 106, 'subsample': 0.405329343331873}, train score = 0.932546072642691, test score = 0.8640815778880988\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6439444508472913, 'gamma': 0.49289699348020904, 'learning_rate': 0.19088247112046844, 'max_depth': 4, 'n_estimators': 111, 'subsample': 0.5636269575835422}, train score = 0.8965378421900161, test score = 0.855896954246612\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3055807400400312, 'gamma': 0.309950290961121, 'learning_rate': 0.2305737498725282, 'max_depth': 5, 'n_estimators': 137, 'subsample': 0.4246068103425968}, train score = 0.9318751118268026, test score = 0.8655574936267275\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.47205931649328914, 'gamma': 0.2520474500722088, 'learning_rate': 0.2871749273323586, 'max_depth': 5, 'n_estimators': 144, 'subsample': 0.5515106522016212}, train score = 0.9539720880300591, test score = 0.874547162216557\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5003713755483365, 'gamma': 0.014271505716483834, 'learning_rate': 0.1844993362169992, 'max_depth': 3, 'n_estimators': 146, 'subsample': 0.41501088639071093}, train score = 0.8821792807300054, test score = 0.8542868643499262\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.46226113546103986, 'gamma': 0.47565703058607145, 'learning_rate': 0.06389065204746551, 'max_depth': 4, 'n_estimators': 118, 'subsample': 0.5344591648185651}, train score = 0.8404455179817498, test score = 0.8242318529451228\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.587638199167523, 'gamma': 0.33556658824117125, 'learning_rate': 0.23162892008169983, 'max_depth': 2, 'n_estimators': 146, 'subsample': 0.5368389055205116}, train score = 0.8669261048488102, test score = 0.8490540721856971\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3001730757189247, 'gamma': 0.41268315516423004, 'learning_rate': 0.07793639902974916, 'max_depth': 5, 'n_estimators': 122, 'subsample': 0.40944092761657136}, train score = 0.8707729468599034, test score = 0.8431504092311821\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5628994109272792, 'gamma': 0.3503694108005984, 'learning_rate': 0.18473250067878394, 'max_depth': 2, 'n_estimators': 119, 'subsample': 0.4494956551364294}, train score = 0.8467525496511004, test score = 0.8359050046960955\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params = {'colsample_bytree': 0.38711087700349944, 'gamma': 0.2970084687931152, 'learning_rate': 0.10361862721911703, 'max_depth': 4, 'n_estimators': 127, 'subsample': 0.518785489471999}, train score = 0.8702809089282519, test score = 0.8442238024956393\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.32411005369767226, 'gamma': 0.07325119505848582, 'learning_rate': 0.2963620655792649, 'max_depth': 2, 'n_estimators': 122, 'subsample': 0.48772439705311665}, train score = 0.8665235283592772, test score = 0.8489198980276399\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5947497820346047, 'gamma': 0.31121535314086496, 'learning_rate': 0.2430383100800883, 'max_depth': 5, 'n_estimators': 106, 'subsample': 0.5454172576672109}, train score = 0.9272231168366434, test score = 0.8650207969944989\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5813885841263848, 'gamma': 0.27912674541614085, 'learning_rate': 0.17046765650291373, 'max_depth': 2, 'n_estimators': 126, 'subsample': 0.5485655027981216}, train score = 0.8458579352299159, test score = 0.836307527170267\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6981282863771661, 'gamma': 0.2586540651392679, 'learning_rate': 0.05191300758407071, 'max_depth': 5, 'n_estimators': 147, 'subsample': 0.5146110867851287}, train score = 0.8605296117373412, test score = 0.8332215215349523\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5262735288437888, 'gamma': 0.44229457438062253, 'learning_rate': 0.2738627923478978, 'max_depth': 4, 'n_estimators': 104, 'subsample': 0.5852377750658835}, train score = 0.9105832886026123, test score = 0.8618006172011271\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5266044201715832, 'gamma': 0.4373012128028433, 'learning_rate': 0.2177995379159275, 'max_depth': 5, 'n_estimators': 143, 'subsample': 0.5826315381918111}, train score = 0.9395687958489891, test score = 0.8671675835234134\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6922460204844285, 'gamma': 0.2126769599184825, 'learning_rate': 0.08966613783476234, 'max_depth': 4, 'n_estimators': 132, 'subsample': 0.40226935488482873}, train score = 0.863079262837717, test score = 0.8418086676506105\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = {}\n",
    "train_scores = {}\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "for i in range(100):\n",
    "    colsample_bytree = np.random.uniform(0.7, 0.3)\n",
    "    gamma         = np.random.uniform(0, 0.5)\n",
    "    learning_rate = np.random.uniform(0.08, 0.3)\n",
    "    max_depth     = np.random.randint(2, 6)\n",
    "    n_estimators  = np.random.randint(100, 150)\n",
    "    subsample     = np.random.uniform(0.6, 0.4)\n",
    "    params = {\n",
    "     \"colsample_bytree\": colsample_bytree,\n",
    "     \"gamma\": gamma,\n",
    "     \"learning_rate\": learning_rate, # default 0.1 \n",
    "     \"max_depth\": max_depth, # default 3\n",
    "     \"n_estimators\": n_estimators, # default 100\n",
    "     \"subsample\": subsample\n",
    "    }\n",
    "    model=xgb.XGBClassifier(objective=\"multi:softprob\",eval_metric = \"merror\",random_state=1,**params )\n",
    "    model.fit(X_train_bigram_numeric, y_train)\n",
    "    te = model.score(X_test_bigram_numeric,y_test)\n",
    "    tr = model.score(X_train_bigram_numeric, y_train)\n",
    "    test_scores[tuple(params.values())] = te\n",
    "    train_scores[tuple(params.values())] = tr\n",
    "    print(\"Params = {}, train score = {}, test score = {}\\n\\n\".format(params, tr, te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\")\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, X_train.iloc[:, [2,4]]])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, X_test.iloc[:, [2,4]]])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_bigram_numeric_scaled = scaler.fit_transform(X_train_bigram_numeric)\n",
    "X_test_bigram_numeric_scaled  = scaler.transform(X_test_bigram_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score = 0.9998210771157631, test score = 0.8934657185026164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = MultinomialNB()\n",
    "estimators.append(('MulNB', model2))\n",
    "# model2 = DecisionTreeClassifier()\n",
    "# estimators.append(('cart', model2))\n",
    "model3 = LinearSVC(C = 0.00838888888888889)\n",
    "estimators.append(('svm', model3))\n",
    "model4 = RandomForestClassifier(n_estimators = 300)\n",
    "estimators.append(('rf', model4))\n",
    "params = {'colsample_bytree': 0.47205931649328914, 'gamma': 0.2520474500722088, 'learning_rate': 0.2871749273323586, 'max_depth': 5, 'n_estimators': 144, 'subsample': 0.5515106522016212}\n",
    "model5 = xgb.XGBClassifier(objective=\"multi:softprob\",eval_metric = \"merror\",random_state=1,**params )\n",
    "estimators.append(('xgb1', model5))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, n_jobs = -1)\n",
    "\n",
    "ensemble.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "tr = ensemble.score(X_train_bigram_numeric_scaled, y_train)\n",
    "te = ensemble.score(X_test_bigram_numeric_scaled, y_test)\n",
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train_title_bigram = vectorizer.fit_transform(Data_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(Data_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(Data_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(Data_test.abstract)\n",
    "Data_train.references = Data_train.references.fillna(\"\")\n",
    "Data_test.references = Data_test.references.fillna(\"\")\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(Data_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(Data_test.references)\n",
    "from scipy.sparse import hstack\n",
    "real_X_train_bigram_numeric = hstack([X_train_title_bigram, X_train_abstract_bigram, X_train_references_bigram, Data_train.iloc[:, [3,5]]])\n",
    "real_X_test_bigram_numeric  = hstack([X_test_title_bigram, X_test_abstract_bigram, X_test_references_bigram, Data_test.iloc[:, [3,5]]])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "real_X_train_bigram_numeric_scaled = scaler.fit_transform(real_X_train_bigram_numeric)\n",
    "real_X_test_bigram_numeric_scaled  = scaler.transform(real_X_test_bigram_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:,6])\n",
    "prediction = ensemble.predict(real_X_test_bigram_numeric_scaled)\n",
    "prediction = pd.DataFrame(prediction, index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1102_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\")\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()\n",
    "X_train_year_onehot = encode.fit_transform(np.array(X_train.iloc[:,2]).reshape(-1, 1))\n",
    "X_test_year_onehot = encode.transform(np.array(X_test.iloc[:,2]).reshape(-1, 1))\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, \n",
    "                                 X_train_abstract_bigram, \n",
    "                                 X_train_references_bigram, \n",
    "                                 X_train_year_onehot, \n",
    "                                 np.array(X_train.iloc[:, 4]).reshape(-1, 1)])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, \n",
    "                                 X_test_abstract_bigram, \n",
    "                                 X_test_references_bigram, \n",
    "                                 X_test_year_onehot, \n",
    "                                 np.array(X_test.iloc[:, 4]).reshape(-1, 1)])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_bigram_numeric_scaled = scaler.fit_transform(X_train_bigram_numeric)\n",
    "X_test_bigram_numeric_scaled  = scaler.transform(X_test_bigram_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params = {'colsample_bytree': 0.49415695293611894, 'gamma': 0.40494950353418324, 'learning_rate': 0.16336584941488316, 'max_depth': 2, 'n_estimators': 115, 'subsample': 0.4431513819908308}, train score = 0.8377169439971373, test score = 0.8285254260029519\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5239025896736734, 'gamma': 0.2983399602643831, 'learning_rate': 0.21350179889275844, 'max_depth': 5, 'n_estimators': 142, 'subsample': 0.4525243029148969}, train score = 0.9317409196636249, test score = 0.8635448812558701\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5919179976548663, 'gamma': 0.07466046036833285, 'learning_rate': 0.23109068962853985, 'max_depth': 3, 'n_estimators': 144, 'subsample': 0.5804171042985782}, train score = 0.8941671139738773, test score = 0.8585804374077552\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5810435725142817, 'gamma': 0.05878923298268307, 'learning_rate': 0.12474334049375965, 'max_depth': 2, 'n_estimators': 108, 'subsample': 0.5780395348267672}, train score = 0.8213007693684022, test score = 0.8157788809875218\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.32490691651746056, 'gamma': 0.030767370196672417, 'learning_rate': 0.23280624756688945, 'max_depth': 2, 'n_estimators': 112, 'subsample': 0.48031257111900405}, train score = 0.8539094650205762, test score = 0.8381859653830672\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3979828482786247, 'gamma': 0.04975254055108008, 'learning_rate': 0.1215092546809256, 'max_depth': 4, 'n_estimators': 145, 'subsample': 0.4803235889497259}, train score = 0.8819108964036501, test score = 0.8503958137662686\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5632492091571009, 'gamma': 0.1382757942189063, 'learning_rate': 0.2896789775097093, 'max_depth': 3, 'n_estimators': 110, 'subsample': 0.5619936677897288}, train score = 0.8899176954732511, test score = 0.854152690191869\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5879298328287825, 'gamma': 0.4262320167157016, 'learning_rate': 0.20172533453070163, 'max_depth': 5, 'n_estimators': 125, 'subsample': 0.508773904790987}, train score = 0.9240472356414385, test score = 0.8587146115658124\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4496117151582616, 'gamma': 0.2679027727653259, 'learning_rate': 0.25363178229551187, 'max_depth': 3, 'n_estimators': 106, 'subsample': 0.4989931885428054}, train score = 0.8788244766505636, test score = 0.8497249429759828\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6501982930881989, 'gamma': 0.37009499951844266, 'learning_rate': 0.2990381303135647, 'max_depth': 4, 'n_estimators': 111, 'subsample': 0.47777012513577427}, train score = 0.9163088208981929, test score = 0.8588487857238696\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.46202177090442387, 'gamma': 0.21559774621250838, 'learning_rate': 0.1292955085733416, 'max_depth': 2, 'n_estimators': 118, 'subsample': 0.4051587581307383}, train score = 0.8277419932009304, test score = 0.8200724540453509\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.38234711056209625, 'gamma': 0.1202371763746593, 'learning_rate': 0.16452989085531178, 'max_depth': 5, 'n_estimators': 139, 'subsample': 0.41848820152506916}, train score = 0.9128645553766327, test score = 0.8589829598819267\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.658379374729618, 'gamma': 0.45505889435816044, 'learning_rate': 0.14969121271823913, 'max_depth': 3, 'n_estimators': 120, 'subsample': 0.5402344574384802}, train score = 0.8631687242798354, test score = 0.8403327519119818\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3757441732931024, 'gamma': 0.0673498065241685, 'learning_rate': 0.09584028167405882, 'max_depth': 2, 'n_estimators': 108, 'subsample': 0.4167479791461027}, train score = 0.8064054392556808, test score = 0.8020931168656916\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.639900529585913, 'gamma': 0.41361926072282157, 'learning_rate': 0.11965594067388538, 'max_depth': 4, 'n_estimators': 110, 'subsample': 0.474928173705224}, train score = 0.8671050277330471, test score = 0.8410036227022676\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6103815827383826, 'gamma': 0.32946302858345367, 'learning_rate': 0.10554149542948424, 'max_depth': 4, 'n_estimators': 137, 'subsample': 0.47084389281752126}, train score = 0.8727858293075684, test score = 0.8440896283375822\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.41249463205652837, 'gamma': 0.08289071460699848, 'learning_rate': 0.1539684577204088, 'max_depth': 3, 'n_estimators': 115, 'subsample': 0.4710186336000538}, train score = 0.8607979960636966, test score = 0.8397960552797531\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.30063250519833384, 'gamma': 0.2595940447432682, 'learning_rate': 0.14600876185156136, 'max_depth': 5, 'n_estimators': 118, 'subsample': 0.5551537886218703}, train score = 0.8998479155483986, test score = 0.8579095666174694\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.33980270899214554, 'gamma': 0.41620841999891384, 'learning_rate': 0.15788133325580672, 'max_depth': 5, 'n_estimators': 122, 'subsample': 0.5746378525795591}, train score = 0.9083467525496511, test score = 0.8583120890916409\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6467139227419343, 'gamma': 0.2685405779991978, 'learning_rate': 0.29863481839172423, 'max_depth': 4, 'n_estimators': 144, 'subsample': 0.42920569761137983}, train score = 0.9299516908212561, test score = 0.8627398363075272\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6970091475141742, 'gamma': 0.21371862143187093, 'learning_rate': 0.17320584795556737, 'max_depth': 5, 'n_estimators': 105, 'subsample': 0.4025341811164731}, train score = 0.9041867954911433, test score = 0.854957735140212\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.32233744497142536, 'gamma': 0.24787283464357068, 'learning_rate': 0.0981033909609639, 'max_depth': 5, 'n_estimators': 104, 'subsample': 0.5756483020904082}, train score = 0.8727858293075684, test score = 0.8410036227022676\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.34298980536108203, 'gamma': 0.042309321016212265, 'learning_rate': 0.1285154897765983, 'max_depth': 3, 'n_estimators': 144, 'subsample': 0.4288150483498371}, train score = 0.8610663803900519, test score = 0.8435529317053535\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5778893410696997, 'gamma': 0.3465033913471024, 'learning_rate': 0.19569347058712433, 'max_depth': 5, 'n_estimators': 118, 'subsample': 0.5944478663520766}, train score = 0.9199767400250493, test score = 0.8576412183013552\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.41006844266540254, 'gamma': 0.1598360963885832, 'learning_rate': 0.10703694605914432, 'max_depth': 5, 'n_estimators': 110, 'subsample': 0.4234941096985362}, train score = 0.879495437466452, test score = 0.8490540721856971\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5188677966940868, 'gamma': 0.028575425432446844, 'learning_rate': 0.17515165395332538, 'max_depth': 4, 'n_estimators': 147, 'subsample': 0.5457624715058246}, train score = 0.9040078726069064, test score = 0.8611297464108413\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3508637409026596, 'gamma': 0.23895611365228736, 'learning_rate': 0.08199895149741773, 'max_depth': 3, 'n_estimators': 121, 'subsample': 0.4854181116494309}, train score = 0.8314993737699051, test score = 0.822755937206494\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4786044059762691, 'gamma': 0.16073386776864512, 'learning_rate': 0.21102649844723703, 'max_depth': 2, 'n_estimators': 132, 'subsample': 0.5418612372430072}, train score = 0.8555197709787081, test score = 0.8407352743861533\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4549245094127602, 'gamma': 0.49928694515883837, 'learning_rate': 0.09354224740782983, 'max_depth': 3, 'n_estimators': 109, 'subsample': 0.4562730824800848}, train score = 0.8319914117015567, test score = 0.8232926338387226\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6388672624573583, 'gamma': 0.18749213513783447, 'learning_rate': 0.09573909030074607, 'max_depth': 4, 'n_estimators': 134, 'subsample': 0.43633702359852217}, train score = 0.8659867597065665, test score = 0.8400644035958674\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.44502142770401343, 'gamma': 0.1328194974962103, 'learning_rate': 0.1933791205124578, 'max_depth': 5, 'n_estimators': 139, 'subsample': 0.522701905607681}, train score = 0.9252549651100376, test score = 0.8632765329397558\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6622246047472884, 'gamma': 0.2157030570522417, 'learning_rate': 0.27211908971627574, 'max_depth': 2, 'n_estimators': 143, 'subsample': 0.527160755282896}, train score = 0.8730542136339238, test score = 0.8507983362404401\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params = {'colsample_bytree': 0.4673720171973651, 'gamma': 0.015265040656822937, 'learning_rate': 0.19437910230823707, 'max_depth': 5, 'n_estimators': 111, 'subsample': 0.4154449708018563}, train score = 0.9116120951869744, test score = 0.8580437407755266\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.48947291923779734, 'gamma': 0.39493905384137173, 'learning_rate': 0.11155628743708404, 'max_depth': 4, 'n_estimators': 109, 'subsample': 0.5474925259400785}, train score = 0.8612005725532296, test score = 0.8356366563799812\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.44027932682065674, 'gamma': 0.42783790467091704, 'learning_rate': 0.18529528617422542, 'max_depth': 4, 'n_estimators': 129, 'subsample': 0.5976504972716395}, train score = 0.8989085704061549, test score = 0.8583120890916409\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4661797542514529, 'gamma': 0.18517650139399844, 'learning_rate': 0.22115666372676362, 'max_depth': 3, 'n_estimators': 106, 'subsample': 0.5926853896463291}, train score = 0.8758722490606549, test score = 0.8461022407084395\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3901388577269045, 'gamma': 0.17201855538203964, 'learning_rate': 0.25401729260722217, 'max_depth': 3, 'n_estimators': 129, 'subsample': 0.4401083690566735}, train score = 0.8896493111468957, test score = 0.857507044143298\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5589899681477039, 'gamma': 0.3377231473247383, 'learning_rate': 0.08487127346879639, 'max_depth': 2, 'n_estimators': 129, 'subsample': 0.5762025985754995}, train score = 0.8110574342458401, test score = 0.8053132966590635\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.30240183504697776, 'gamma': 0.3444106274395256, 'learning_rate': 0.18540853962062748, 'max_depth': 4, 'n_estimators': 105, 'subsample': 0.527797177772203}, train score = 0.8861603149042763, test score = 0.8526767744532403\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3870651765951135, 'gamma': 0.29008479713406976, 'learning_rate': 0.13017518784640597, 'max_depth': 4, 'n_estimators': 136, 'subsample': 0.4642416865097594}, train score = 0.8815530506351762, test score = 0.8489198980276399\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4830565483180541, 'gamma': 0.17943156393307963, 'learning_rate': 0.12276374674092702, 'max_depth': 3, 'n_estimators': 130, 'subsample': 0.4613947808232719}, train score = 0.8554303095365897, test score = 0.8379176170669529\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4600624078552978, 'gamma': 0.4513717709065102, 'learning_rate': 0.19607105260882662, 'max_depth': 4, 'n_estimators': 131, 'subsample': 0.5696753646756633}, train score = 0.9032027196278404, test score = 0.8593854823560982\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5827258820556586, 'gamma': 0.35901874756209784, 'learning_rate': 0.14605443814671504, 'max_depth': 4, 'n_estimators': 127, 'subsample': 0.4217258216663816}, train score = 0.8848631239935588, test score = 0.8479806789212397\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5335976702458938, 'gamma': 0.399772696949357, 'learning_rate': 0.12426441365678821, 'max_depth': 2, 'n_estimators': 109, 'subsample': 0.5772020145045919}, train score = 0.8217928073000537, test score = 0.8152421843552932\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4677106122927382, 'gamma': 0.26327048063766134, 'learning_rate': 0.09562188911187276, 'max_depth': 5, 'n_estimators': 110, 'subsample': 0.5736320399528342}, train score = 0.8755144032921811, test score = 0.8432845833892393\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6807114107433611, 'gamma': 0.26812051940061954, 'learning_rate': 0.26992826613607884, 'max_depth': 3, 'n_estimators': 149, 'subsample': 0.5318716546478771}, train score = 0.9032027196278404, test score = 0.8618006172011271\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3309188795811899, 'gamma': 0.29523809463292955, 'learning_rate': 0.2549992184487582, 'max_depth': 5, 'n_estimators': 128, 'subsample': 0.5524357648110912}, train score = 0.9348273394167114, test score = 0.8648866228364417\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.33725740093215045, 'gamma': 0.4901337902122655, 'learning_rate': 0.14142487342887125, 'max_depth': 3, 'n_estimators': 148, 'subsample': 0.5763686797470147}, train score = 0.8675076042225801, test score = 0.8448946732859252\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5215783748400359, 'gamma': 0.47177525559679545, 'learning_rate': 0.16006511406711682, 'max_depth': 5, 'n_estimators': 126, 'subsample': 0.4558869797216004}, train score = 0.9099123277867239, test score = 0.8565678250368979\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6013501818902776, 'gamma': 0.25152744440678515, 'learning_rate': 0.1442232066117382, 'max_depth': 5, 'n_estimators': 101, 'subsample': 0.5469879807789793}, train score = 0.8934514224369297, test score = 0.8516033811887831\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.41931774852472126, 'gamma': 0.08397421691154927, 'learning_rate': 0.25063217159445417, 'max_depth': 3, 'n_estimators': 129, 'subsample': 0.49324550482814217}, train score = 0.889112542494185, test score = 0.8550919092982692\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.44263214407383017, 'gamma': 0.39314644300849255, 'learning_rate': 0.09897168315601484, 'max_depth': 4, 'n_estimators': 138, 'subsample': 0.5336897799538474}, train score = 0.8683127572016461, test score = 0.8424795384408963\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6068910556944019, 'gamma': 0.2214073681726229, 'learning_rate': 0.09385070365721676, 'max_depth': 5, 'n_estimators': 101, 'subsample': 0.46622850836563323}, train score = 0.8692521023438898, test score = 0.8393935328055816\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.42279021806615347, 'gamma': 0.012655548049094967, 'learning_rate': 0.2736942002107522, 'max_depth': 2, 'n_estimators': 121, 'subsample': 0.46230126434407015}, train score = 0.8637949543746645, test score = 0.8474439822890112\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6857080155842653, 'gamma': 0.3003250235521291, 'learning_rate': 0.18331146237722856, 'max_depth': 2, 'n_estimators': 141, 'subsample': 0.5219047001794197}, train score = 0.8543567722311683, test score = 0.8361733530122099\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4282998134682748, 'gamma': 0.383724009114124, 'learning_rate': 0.272249383840066, 'max_depth': 2, 'n_estimators': 127, 'subsample': 0.4266873142869864}, train score = 0.8659420289855072, test score = 0.8461022407084395\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.47401782648591106, 'gamma': 0.030949416689219744, 'learning_rate': 0.19161075699013996, 'max_depth': 2, 'n_estimators': 112, 'subsample': 0.5059007234963739}, train score = 0.8446949364823761, test score = 0.8328189990607809\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5594257873340954, 'gamma': 0.17948448118288507, 'learning_rate': 0.2164604928736173, 'max_depth': 3, 'n_estimators': 145, 'subsample': 0.40698150038097325}, train score = 0.8884415816782967, test score = 0.8528109486112975\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.378889905818135, 'gamma': 0.10954833630186328, 'learning_rate': 0.19828973144497442, 'max_depth': 4, 'n_estimators': 133, 'subsample': 0.4263371955924029}, train score = 0.9026212202540705, test score = 0.8565678250368979\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5987276350383852, 'gamma': 0.07786801676607374, 'learning_rate': 0.15408419022264813, 'max_depth': 3, 'n_estimators': 123, 'subsample': 0.5125766693843301}, train score = 0.8646001073537305, test score = 0.8397960552797531\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5276034114357226, 'gamma': 0.19966723338046027, 'learning_rate': 0.1974956523853701, 'max_depth': 5, 'n_estimators': 117, 'subsample': 0.4274497867387571}, train score = 0.9164430130613705, test score = 0.8589829598819267\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.36019157011437103, 'gamma': 0.07141219219690376, 'learning_rate': 0.10920362878943887, 'max_depth': 4, 'n_estimators': 140, 'subsample': 0.5417226763948559}, train score = 0.8749329039184112, test score = 0.8454313699181538\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6929834844923601, 'gamma': 0.3592563439503159, 'learning_rate': 0.25916703542301317, 'max_depth': 5, 'n_estimators': 141, 'subsample': 0.41865420270070897}, train score = 0.94296833064949, test score = 0.8655574936267275\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5265794379079913, 'gamma': 0.41024271473560603, 'learning_rate': 0.15230887895901757, 'max_depth': 5, 'n_estimators': 101, 'subsample': 0.4615689879891297}, train score = 0.8934514224369297, test score = 0.8525426002951831\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params = {'colsample_bytree': 0.5299677815756025, 'gamma': 0.019910690002598763, 'learning_rate': 0.14519599162090369, 'max_depth': 2, 'n_estimators': 138, 'subsample': 0.42248786092695745}, train score = 0.8421900161030595, test score = 0.8321481282704951\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5909903549675323, 'gamma': 0.29852809661171925, 'learning_rate': 0.09897516959465198, 'max_depth': 2, 'n_estimators': 122, 'subsample': 0.5536205953726864}, train score = 0.8161567364465915, test score = 0.8109486112974641\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.31627065012396266, 'gamma': 0.3151098460505343, 'learning_rate': 0.2192439741390398, 'max_depth': 2, 'n_estimators': 124, 'subsample': 0.44086343288257135}, train score = 0.853283234925747, test score = 0.8411377968603247\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5854012895805467, 'gamma': 0.4693077232916942, 'learning_rate': 0.17982887811854487, 'max_depth': 3, 'n_estimators': 115, 'subsample': 0.5321720386897326}, train score = 0.8695652173913043, test score = 0.8435529317053535\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5979571305086352, 'gamma': 0.25004548773712837, 'learning_rate': 0.24895942922838693, 'max_depth': 2, 'n_estimators': 114, 'subsample': 0.48502134678424974}, train score = 0.8585614600107354, test score = 0.8418086676506105\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6146287642998903, 'gamma': 0.49569277794673955, 'learning_rate': 0.25167195872492076, 'max_depth': 3, 'n_estimators': 130, 'subsample': 0.4628651057910853}, train score = 0.8920200393630345, test score = 0.8568361733530122\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6479431598560335, 'gamma': 0.47317918570015943, 'learning_rate': 0.1879548667839993, 'max_depth': 2, 'n_estimators': 104, 'subsample': 0.5400935344263686}, train score = 0.8420110932188227, test score = 0.8282570776868375\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5937302974725337, 'gamma': 0.27463462042569586, 'learning_rate': 0.12988757433337136, 'max_depth': 3, 'n_estimators': 117, 'subsample': 0.5264399910219414}, train score = 0.8526570048309179, test score = 0.836307527170267\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4247803507221282, 'gamma': 0.30018666562944163, 'learning_rate': 0.23676541674401924, 'max_depth': 2, 'n_estimators': 134, 'subsample': 0.4894628260195931}, train score = 0.8620504562533549, test score = 0.8442238024956393\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.4283026673172006, 'gamma': 0.4575460577474933, 'learning_rate': 0.27406689950570046, 'max_depth': 2, 'n_estimators': 134, 'subsample': 0.48016911814086566}, train score = 0.8666577205224548, test score = 0.8470414598148397\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6346721669485663, 'gamma': 0.3151057860971518, 'learning_rate': 0.14272214549885115, 'max_depth': 3, 'n_estimators': 146, 'subsample': 0.5514839111554615}, train score = 0.8704151010914296, test score = 0.8450288474439823\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.38161043049169774, 'gamma': 0.16183947412165078, 'learning_rate': 0.14384987273724553, 'max_depth': 2, 'n_estimators': 126, 'subsample': 0.5267564035852592}, train score = 0.8351225621757022, test score = 0.828659600161009\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.5549360768396318, 'gamma': 0.4498676217015672, 'learning_rate': 0.14932389391866555, 'max_depth': 5, 'n_estimators': 143, 'subsample': 0.5859803105274958}, train score = 0.9159957058507783, test score = 0.8596538306722126\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.6291323865751838, 'gamma': 0.14487158139448603, 'learning_rate': 0.2195063727846761, 'max_depth': 3, 'n_estimators': 136, 'subsample': 0.46355261506870094}, train score = 0.8862945070674539, test score = 0.8521400778210116\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3389036865900834, 'gamma': 0.12732684507356878, 'learning_rate': 0.21771241073357672, 'max_depth': 5, 'n_estimators': 122, 'subsample': 0.49769334689250594}, train score = 0.9219896224727142, test score = 0.8648866228364417\n",
      "\n",
      "\n",
      "Params = {'colsample_bytree': 0.3883800844368352, 'gamma': 0.07222122921449137, 'learning_rate': 0.14916523264390708, 'max_depth': 5, 'n_estimators': 103, 'subsample': 0.414545912418546}, train score = 0.8922436929683306, test score = 0.8530792969274118\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = {}\n",
    "train_scores = {}\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "for i in range(100):\n",
    "    colsample_bytree = np.random.uniform(0.7, 0.3)\n",
    "    gamma         = np.random.uniform(0, 0.5)\n",
    "    learning_rate = np.random.uniform(0.08, 0.3)\n",
    "    max_depth     = np.random.randint(2, 6)\n",
    "    n_estimators  = np.random.randint(100, 150)\n",
    "    subsample     = np.random.uniform(0.6, 0.4)\n",
    "    params = {\n",
    "     \"colsample_bytree\": colsample_bytree,\n",
    "     \"gamma\": gamma,\n",
    "     \"learning_rate\": learning_rate, # default 0.1 \n",
    "     \"max_depth\": max_depth, # default 3\n",
    "     \"n_estimators\": n_estimators, # default 100\n",
    "     \"subsample\": subsample\n",
    "    }\n",
    "    model=xgb.XGBClassifier(objective=\"multi:softprob\",eval_metric = \"merror\",random_state=1997,**params )\n",
    "    model.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "    te = model.score(X_test_bigram_numeric_scaled,y_test)\n",
    "    tr = model.score(X_train_bigram_numeric_scaled, y_train)\n",
    "    test_scores[tuple(params.values())] = te\n",
    "    train_scores[tuple(params.values())] = tr\n",
    "    print(\"Params = {}, train score = {}, test score = {}\\n\\n\".format(params, tr, te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## itf + GSD + ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Data_train = pd.read_csv(\"train.csv\")\n",
    "Data_test  = pd.read_csv(\"test.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_train.iloc[:,1:6], Data_train.iloc[:,6],random_state = 0, stratify = Data_train.iloc[:,6])\n",
    "\n",
    "\n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, random_state = 0, stratify = y_train)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 3))\n",
    "\n",
    "X_train_title_bigram = vectorizer.fit_transform(X_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(X_test.title)\n",
    "\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(X_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(X_test.abstract)\n",
    "\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train.references = X_train.references.fillna(\"\")\n",
    "X_test.references = X_test.references.fillna(\"\")\n",
    "X_train_references_bigram = vectorizer.fit_transform(X_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(X_test.references)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()\n",
    "X_train_year_onehot = encode.fit_transform(np.array(X_train.iloc[:,2]).reshape(-1,1))\n",
    "X_test_year_onehot = encode.transform(np.array(X_test.iloc[:,2]).reshape(-1,1))\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_year = poly.fit_transform(np.array(X_train.iloc[:,2]).reshape(-1,1))\n",
    "X_test_year  = poly.transform(np.array(X_test.iloc[:,2]).reshape(-1,1))\n",
    "\n",
    "X_train_n_citation = poly.fit_transform(np.array(X_train.iloc[:,4]).reshape(-1,1))\n",
    "X_test_n_citation  = poly.transform(np.array(X_test.iloc[:,4]).reshape(-1,1))\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "X_train_bigram_numeric = hstack([X_train_title_bigram, \n",
    "                                 X_train_abstract_bigram, \n",
    "                                 X_train_references_bigram, \n",
    "                                 X_train_year_onehot, \n",
    "                                 X_train_year,\n",
    "                                 X_train_n_citation])\n",
    "X_test_bigram_numeric  = hstack([X_test_title_bigram, \n",
    "                                 X_test_abstract_bigram, \n",
    "                                 X_test_references_bigram, \n",
    "                                 X_test_year_onehot, \n",
    "                                 X_test_year,\n",
    "                                 X_test_n_citation])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_bigram_numeric_scaled = scaler.fit_transform(X_train_bigram_numeric)\n",
    "X_test_bigram_numeric_scaled  = scaler.transform(X_test_bigram_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score = 1.0, test score = 0.894002415134845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# import xgboost as xgb\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "#model2 = MultinomialNB()\n",
    "#estimators.append(('MulNB', model2))\n",
    "model41 = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "estimators.append(('SGD1', model41))\n",
    "model42 = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=1e-2, random_state=42, max_iter=5, tol=None)\n",
    "estimators.append(('SGD2', model42))\n",
    "model43 = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=0.0025, random_state=40, max_iter=5, tol=None)\n",
    "estimators.append(('SGD3', model43))\n",
    "\n",
    "model3 = LinearSVC(C = 0.01)\n",
    "estimators.append(('svm', model3))\n",
    "model3_1 = LinearSVC(C = 1)\n",
    "estimators.append(('svm1', model3_1))\n",
    "# model4 = RandomForestClassifier(n_estimators = 300)\n",
    "# estimators.append(('rf', model4))\n",
    "# params = {'colsample_bytree': 0.47205931649328914, 'gamma': 0.2520474500722088, 'learning_rate': 0.2871749273323586, 'max_depth': 5, 'n_estimators': 144, 'subsample': 0.5515106522016212}\n",
    "# model5 = xgb.XGBClassifier(objective=\"multi:softprob\",eval_metric = \"merror\",random_state=1,**params )\n",
    "# estimators.append(('xgb1', model5))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, n_jobs = -1)\n",
    "\n",
    "ensemble.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "tr = ensemble.score(X_train_bigram_numeric_scaled, y_train)\n",
    "te = ensemble.score(X_test_bigram_numeric_scaled, y_test)\n",
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53564703\n",
      "Iteration 2, loss = 0.00461451\n",
      "Iteration 3, loss = 0.00308172\n",
      "Iteration 4, loss = 0.00269152\n",
      "Iteration 5, loss = 0.00242104\n",
      "Iteration 6, loss = 0.00228914\n",
      "Iteration 7, loss = 0.00219043\n",
      "Iteration 8, loss = 0.00209872\n",
      "Iteration 9, loss = 0.00202406\n",
      "Iteration 10, loss = 0.00195738\n",
      "Iteration 11, loss = 0.00189530\n",
      "Iteration 12, loss = 0.00183761\n",
      "Iteration 13, loss = 0.00178106\n",
      "Iteration 14, loss = 0.00172807\n",
      "Iteration 15, loss = 0.00167686\n",
      "Iteration 16, loss = 0.00162814\n",
      "Iteration 17, loss = 0.00158072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'this_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ecce3e00952f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_bigram_numeric_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#scores.append(te)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"size = {},train score = {}, test score = {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#n_iter_no_change=10, 0.901\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'this_size' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#scores = []\n",
    "#for this_size in range(19,36):\n",
    "model5 = MLPClassifier(hidden_layer_sizes=(50), \n",
    "                       random_state = 0, \n",
    "                       verbose = True) \n",
    "                       #n_iter_no_change=5,\n",
    "                       #learning_rate_init = 0.01,\n",
    "                       #early_stopping=True)\n",
    "model5.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "tr = model5.score(X_train_bigram_numeric_scaled, y_train)\n",
    "te = model5.score(X_test_bigram_numeric_scaled, y_test)\n",
    "#scores.append(te)\n",
    "print(\"size = {},train score = {}, test score = {}\\n\".format(this_size, tr,te))\n",
    "\n",
    "#n_iter_no_change=10, 0.901\n",
    "#n_iter_no_change=5, 0.9001\n",
    "#n_iter_no_change=3， 0.899\n",
    "# 2， 0.899\n",
    "# 1， 0.899\n",
    "\n",
    "# 1，0.01，\n",
    "\n",
    "# 4,8:0.88\n",
    "# 6,8:0.89\n",
    "# 12,8:0.898\n",
    "# 4,12,8: 0.86\n",
    "# 12,12,8: 0.88\n",
    "# 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score = 1.0, test score = 0.8917214544478733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75377184\n",
      "Iteration 1, loss = 0.71557782\n",
      "Iteration 1, loss = 0.77978001\n",
      "Iteration 2, loss = 0.05753910\n",
      "Iteration 2, loss = 0.07186726\n",
      "Iteration 2, loss = 0.04287427\n",
      "Iteration 3, loss = 0.00895958\n",
      "Iteration 3, loss = 0.00901511\n",
      "Iteration 3, loss = 0.00806472\n",
      "Iteration 4, loss = 0.00391338\n",
      "Iteration 4, loss = 0.00331034\n",
      "Iteration 4, loss = 0.00355714\n",
      "Iteration 5, loss = 0.00263973\n",
      "Iteration 5, loss = 0.00208084\n",
      "Iteration 5, loss = 0.00230595\n",
      "Iteration 6, loss = 0.00202835\n",
      "Iteration 6, loss = 0.00174534\n",
      "Iteration 6, loss = 0.00159948\n",
      "Iteration 7, loss = 0.00169606\n",
      "Iteration 7, loss = 0.00148458\n",
      "Iteration 7, loss = 0.00134455\n",
      "Iteration 8, loss = 0.00148589\n",
      "Iteration 8, loss = 0.00132293\n",
      "Iteration 8, loss = 0.00118749\n",
      "Iteration 9, loss = 0.00134156\n",
      "Iteration 9, loss = 0.00121228\n",
      "Iteration 9, loss = 0.00108085\n",
      "Iteration 10, loss = 0.00123603\n",
      "Iteration 10, loss = 0.00113094\n",
      "Iteration 10, loss = 0.00100346\n",
      "Iteration 11, loss = 0.00115529\n",
      "Iteration 11, loss = 0.00094426\n",
      "Iteration 11, loss = 0.00106777\n",
      "Iteration 12, loss = 0.00109113\n",
      "Iteration 12, loss = 0.00089718\n",
      "Iteration 12, loss = 0.00101721\n",
      "Iteration 13, loss = 0.00103850\n",
      "Iteration 13, loss = 0.00085876\n",
      "Iteration 13, loss = 0.00097467\n",
      "Iteration 14, loss = 0.00099455\n",
      "Iteration 14, loss = 0.00082627\n",
      "Iteration 14, loss = 0.00093885\n",
      "Iteration 15, loss = 0.00095690\n",
      "Iteration 15, loss = 0.00079826\n",
      "Iteration 15, loss = 0.00090720\n",
      "Iteration 16, loss = 0.00092385\n",
      "Iteration 16, loss = 0.00077339\n",
      "Iteration 16, loss = 0.00087910\n",
      "Iteration 17, loss = 0.00089473\n",
      "Iteration 17, loss = 0.00075120\n",
      "Iteration 17, loss = 0.00085386\n",
      "Iteration 18, loss = 0.00086848\n",
      "Iteration 18, loss = 0.00073095\n",
      "Iteration 18, loss = 0.00083080\n",
      "Iteration 19, loss = 0.00084454\n",
      "Iteration 19, loss = 0.00071250\n",
      "Iteration 19, loss = 0.00080941\n",
      "Iteration 20, loss = 0.00082257\n",
      "Iteration 20, loss = 0.00069527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "size = 17,train score = 1.0, test score = 0.8989668589829599\n",
      "\n",
      "Iteration 20, loss = 0.00078956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "size = 18,train score = 1.0, test score = 0.9005769488796458\n",
      "\n",
      "Iteration 21, loss = 0.00080208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "size = 16,train score = 1.0, test score = 0.9000402522474171\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-db34a4037fb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"threading\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def myfun(arg):\n",
    "    model5 = MLPClassifier(hidden_layer_sizes=(arg), random_state = 0, verbose = True)\n",
    "    model5.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "    tr = model5.score(X_train_bigram_numeric_scaled, y_train)\n",
    "    te = model5.score(X_test_bigram_numeric_scaled, y_test)\n",
    "    print(\"size = {},train score = {}, test score = {}\\n\".format(arg, tr,te))\n",
    "    return te\n",
    "\n",
    "#results = Parallel(n_jobs=3, verbose=True, backend=\"threading\")(delayed(myfun)(i) for i in range(16,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5)  : 0.897\n",
    "(8)m : 0.897\n",
    "(8)  ： 0.898\n",
    "（8）0.0001：0.896 \n",
    "(10) : default 0.89// random  0.90205\n",
    "（11）：0.8996\n",
    "（20）：0.9001\n",
    "(2, 5): not default 0.73\n",
    "(2,5) : default 0.81\n",
    "(2,10) : 0.76\n",
    "(8,4)m : 0.86\n",
    "(8,6) : 0.89\n",
    "（10，10）：0.8988\n",
    "(10,2) : 0.84\n",
    "（3，3）m： 0.83\n",
    "\n",
    "above 0.9: 10，11，14，15，16，18,20，22和20一样，24，26，28，29，31(高一点0.901)，\n",
    "32，33，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\",ngram_range=(1, 2))\n",
    "X_train_title_bigram = vectorizer.fit_transform(Data_train.title)\n",
    "X_test_title_bigram  = vectorizer.transform(Data_test.title)\n",
    "X_train_abstract_bigram = vectorizer.fit_transform(Data_train.abstract)\n",
    "X_test_abstract_bigram  = vectorizer.transform(Data_test.abstract)\n",
    "Data_train.references = Data_train.references.fillna(\"\")\n",
    "Data_test.references = Data_test.references.fillna(\"\")\n",
    "\n",
    "\n",
    "X_train_references_bigram = vectorizer.fit_transform(Data_train.references)\n",
    "X_test_references_bigram  = vectorizer.transform(Data_test.references)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()\n",
    "X_train_year_onehot = encode.fit_transform(np.array(Data_train.year).reshape(-1,1))\n",
    "X_test_year_onehot = encode.transform(np.array(Data_test.year).reshape(-1,1))\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_year = poly.fit_transform(np.array(Data_train.year).reshape(-1,1))\n",
    "X_test_year  = poly.transform(np.array(Data_test.year).reshape(-1,1))\n",
    "\n",
    "X_train_n_citation = poly.fit_transform(np.array(Data_train.n_citation).reshape(-1,1))\n",
    "X_test_n_citation  = poly.transform(np.array(Data_test.n_citation).reshape(-1,1))\n",
    "\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "real_X_train_bigram_numeric = hstack([X_train_title_bigram, \n",
    "                                      X_train_abstract_bigram, \n",
    "                                      X_train_references_bigram, \n",
    "                                      X_train_year_onehot,\n",
    "                                      X_train_year,\n",
    "                                      X_train_n_citation])\n",
    "real_X_test_bigram_numeric  = hstack([X_test_title_bigram, \n",
    "                                      X_test_abstract_bigram, \n",
    "                                      X_test_references_bigram, \n",
    "                                      X_test_year_onehot,\n",
    "                                      X_test_year,\n",
    "                                      X_test_n_citation])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "real_X_train_bigram_numeric_scaled = scaler.fit_transform(real_X_train_bigram_numeric)\n",
    "real_X_test_bigram_numeric_scaled  = scaler.transform(real_X_test_bigram_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65204463\n",
      "Iteration 2, loss = 0.03881700\n",
      "Iteration 3, loss = 0.00813349\n",
      "Iteration 4, loss = 0.00473853\n",
      "Iteration 5, loss = 0.00335501\n",
      "Iteration 6, loss = 0.00254591\n",
      "Iteration 7, loss = 0.00207329\n",
      "Iteration 8, loss = 0.00176021\n",
      "Iteration 9, loss = 0.00153931\n",
      "Iteration 10, loss = 0.00137764\n",
      "Iteration 11, loss = 0.00125555\n",
      "Iteration 12, loss = 0.00116023\n",
      "Iteration 13, loss = 0.00108419\n",
      "Iteration 14, loss = 0.00102176\n",
      "Iteration 15, loss = 0.00097010\n",
      "Iteration 16, loss = 0.00092610\n",
      "Iteration 17, loss = 0.00088823\n",
      "Iteration 18, loss = 0.00085523\n",
      "Iteration 19, loss = 0.00082577\n",
      "Iteration 20, loss = 0.00079938\n",
      "Iteration 21, loss = 0.00077540\n",
      "Iteration 22, loss = 0.00075341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tocsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9a5418441353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_X_train_bigram_numeric_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_X_test_bigram_numeric_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prediction1103_v1_nn10.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tocsv'"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model5 = MLPClassifier(hidden_layer_sizes=(10), \n",
    "                       verbose = True,random_state = 0) \n",
    "model5.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(model5.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1103_v1_nn10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score = 1.0, test score = 0.9001744264054743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import xgboost as xgb\n",
    "estimators = []\n",
    "#model1 = LogisticRegression()\n",
    "#estimators.append(('logistic', model1))\n",
    "# model2 = MultinomialNB()\n",
    "# estimators.append(('MulNB', model2))\n",
    "# model2 = DecisionTreeClassifier()\n",
    "# estimators.append(('cart', model2))\n",
    "# model3 = LinearSVC(C = 0.012345444444444445)\n",
    "# estimators.append(('svm', model3))\n",
    "#model2 = LinearSVC(C = 1)\n",
    "#estimators.append(('svm', model2))\n",
    "#model3 = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        #alpha=1e-2, random_state=42, max_iter=5, tol=None)\n",
    "#estimators.append(('SGD', model3))\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model4_1 = MLPClassifier(hidden_layer_sizes=(10), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN1', model4_1))\n",
    "model4_2 = MLPClassifier(hidden_layer_sizes=(11), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN2', model4_2))\n",
    "model4_3 = MLPClassifier(hidden_layer_sizes=(20), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN3', model4_3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, n_jobs = -1)\n",
    "\n",
    "ensemble.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "tr = ensemble.score(X_train_bigram_numeric_scaled, y_train)\n",
    "te = ensemble.score(X_test_bigram_numeric_scaled, y_test)\n",
    "print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1104_v1_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44423794\n",
      "Iteration 2, loss = 0.00665065\n",
      "Iteration 3, loss = 0.00346165\n",
      "Iteration 4, loss = 0.00301107\n",
      "Iteration 5, loss = 0.00271769\n",
      "Iteration 6, loss = 0.00245250\n",
      "Iteration 7, loss = 0.00230934\n",
      "Iteration 8, loss = 0.00214535\n",
      "Iteration 9, loss = 0.00201858\n",
      "Iteration 10, loss = 0.00191537\n",
      "Iteration 11, loss = 0.00182184\n",
      "Iteration 12, loss = 0.00173481\n",
      "Iteration 13, loss = 0.00164950\n",
      "Iteration 14, loss = 0.00157112\n",
      "Iteration 15, loss = 0.00149687\n",
      "Iteration 16, loss = 0.00142503\n",
      "Iteration 17, loss = 0.00135985\n",
      "Iteration 18, loss = 0.00129396\n",
      "Iteration 19, loss = 0.00123211\n",
      "Iteration 20, loss = 0.00117263\n",
      "Iteration 21, loss = 0.00111642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "model5 = MLPClassifier(#hidden_layer_sizes=(12,12,8), \n",
    "                       random_state = 0, \n",
    "                       verbose = True) \n",
    "                       #n_iter_no_change=5,\n",
    "                       #learning_rate_init = 0.01,\n",
    "                       #early_stopping=True)\n",
    "model5.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(model5.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1107_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65204463\n",
      "Iteration 2, loss = 0.03881700\n",
      "Iteration 3, loss = 0.00813349\n",
      "Iteration 4, loss = 0.00473853\n",
      "Iteration 5, loss = 0.00335501\n",
      "Iteration 6, loss = 0.00254591\n",
      "Iteration 7, loss = 0.00207329\n",
      "Iteration 8, loss = 0.00176021\n",
      "Iteration 9, loss = 0.00153931\n",
      "Iteration 10, loss = 0.00137764\n",
      "Iteration 11, loss = 0.00125555\n",
      "Iteration 12, loss = 0.00116023\n",
      "Iteration 13, loss = 0.00108419\n",
      "Iteration 14, loss = 0.00102176\n",
      "Iteration 15, loss = 0.00097010\n",
      "Iteration 16, loss = 0.00092610\n",
      "Iteration 17, loss = 0.00088823\n",
      "Iteration 18, loss = 0.00085523\n",
      "Iteration 19, loss = 0.00082577\n",
      "Iteration 20, loss = 0.00079938\n",
      "Iteration 21, loss = 0.00077540\n",
      "Iteration 22, loss = 0.00075341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70694764\n",
      "Iteration 2, loss = 0.04636713\n",
      "Iteration 3, loss = 0.00950413\n",
      "Iteration 4, loss = 0.00543616\n",
      "Iteration 5, loss = 0.00370199\n",
      "Iteration 6, loss = 0.00279765\n",
      "Iteration 7, loss = 0.00225898\n",
      "Iteration 8, loss = 0.00191420\n",
      "Iteration 9, loss = 0.00167616\n",
      "Iteration 10, loss = 0.00150363\n",
      "Iteration 11, loss = 0.00137459\n",
      "Iteration 12, loss = 0.00127386\n",
      "Iteration 13, loss = 0.00119382\n",
      "Iteration 14, loss = 0.00112844\n",
      "Iteration 15, loss = 0.00107399\n",
      "Iteration 16, loss = 0.00102761\n",
      "Iteration 17, loss = 0.00098739\n",
      "Iteration 18, loss = 0.00095213\n",
      "Iteration 19, loss = 0.00092065\n",
      "Iteration 20, loss = 0.00089237\n",
      "Iteration 21, loss = 0.00086625\n",
      "Iteration 22, loss = 0.00084199\n",
      "Iteration 23, loss = 0.00081943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68375863\n",
      "Iteration 2, loss = 0.04671390\n",
      "Iteration 3, loss = 0.00735352\n",
      "Iteration 4, loss = 0.00393261\n",
      "Iteration 5, loss = 0.00256686\n",
      "Iteration 6, loss = 0.00200934\n",
      "Iteration 7, loss = 0.00163185\n",
      "Iteration 8, loss = 0.00141841\n",
      "Iteration 9, loss = 0.00125611\n",
      "Iteration 10, loss = 0.00113901\n",
      "Iteration 11, loss = 0.00105449\n",
      "Iteration 12, loss = 0.00099155\n",
      "Iteration 13, loss = 0.00094306\n",
      "Iteration 14, loss = 0.00090212\n",
      "Iteration 15, loss = 0.00086764\n",
      "Iteration 16, loss = 0.00083748\n",
      "Iteration 17, loss = 0.00081022\n",
      "Iteration 18, loss = 0.00078586\n",
      "Iteration 19, loss = 0.00076272\n",
      "Iteration 20, loss = 0.00074148\n",
      "Iteration 21, loss = 0.00072133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63453443\n",
      "Iteration 2, loss = 0.03353107\n",
      "Iteration 3, loss = 0.00688116\n",
      "Iteration 4, loss = 0.00310873\n",
      "Iteration 5, loss = 0.00211864\n",
      "Iteration 6, loss = 0.00167863\n",
      "Iteration 7, loss = 0.00146539\n",
      "Iteration 8, loss = 0.00133025\n",
      "Iteration 9, loss = 0.00123435\n",
      "Iteration 10, loss = 0.00116124\n",
      "Iteration 11, loss = 0.00110134\n",
      "Iteration 12, loss = 0.00105095\n",
      "Iteration 13, loss = 0.00100774\n",
      "Iteration 14, loss = 0.00096949\n",
      "Iteration 15, loss = 0.00093468\n",
      "Iteration 16, loss = 0.00090307\n",
      "Iteration 17, loss = 0.00087377\n",
      "Iteration 18, loss = 0.00084641\n",
      "Iteration 19, loss = 0.00082051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58605465\n",
      "Iteration 2, loss = 0.02610711\n",
      "Iteration 3, loss = 0.00540926\n",
      "Iteration 4, loss = 0.00329924\n",
      "Iteration 5, loss = 0.00245302\n",
      "Iteration 6, loss = 0.00196133\n",
      "Iteration 7, loss = 0.00166843\n",
      "Iteration 8, loss = 0.00147666\n",
      "Iteration 9, loss = 0.00133775\n",
      "Iteration 10, loss = 0.00123486\n",
      "Iteration 11, loss = 0.00115445\n",
      "Iteration 12, loss = 0.00108935\n",
      "Iteration 13, loss = 0.00103551\n",
      "Iteration 14, loss = 0.00098962\n",
      "Iteration 15, loss = 0.00094988\n",
      "Iteration 16, loss = 0.00091463\n",
      "Iteration 17, loss = 0.00088300\n",
      "Iteration 18, loss = 0.00085422\n",
      "Iteration 19, loss = 0.00082762\n",
      "Iteration 20, loss = 0.00080293\n",
      "Iteration 21, loss = 0.00077985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61382537\n",
      "Iteration 2, loss = 0.03455931\n",
      "Iteration 3, loss = 0.00561492\n",
      "Iteration 4, loss = 0.00288142\n",
      "Iteration 5, loss = 0.00186698\n",
      "Iteration 6, loss = 0.00154436\n",
      "Iteration 7, loss = 0.00137818\n",
      "Iteration 8, loss = 0.00126763\n",
      "Iteration 9, loss = 0.00118639\n",
      "Iteration 10, loss = 0.00112169\n",
      "Iteration 11, loss = 0.00106777\n",
      "Iteration 12, loss = 0.00102176\n",
      "Iteration 13, loss = 0.00098113\n",
      "Iteration 14, loss = 0.00094489\n",
      "Iteration 15, loss = 0.00091191\n",
      "Iteration 16, loss = 0.00088160\n",
      "Iteration 17, loss = 0.00085324\n",
      "Iteration 18, loss = 0.00082696\n",
      "Iteration 19, loss = 0.00080175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54499737\n",
      "Iteration 2, loss = 0.01403186\n",
      "Iteration 3, loss = 0.00434587\n",
      "Iteration 4, loss = 0.00285798\n",
      "Iteration 5, loss = 0.00220868\n",
      "Iteration 6, loss = 0.00187647\n",
      "Iteration 7, loss = 0.00166004\n",
      "Iteration 8, loss = 0.00150918\n",
      "Iteration 9, loss = 0.00139670\n",
      "Iteration 10, loss = 0.00130701\n",
      "Iteration 11, loss = 0.00123502\n",
      "Iteration 12, loss = 0.00117517\n",
      "Iteration 13, loss = 0.00112351\n",
      "Iteration 14, loss = 0.00107791\n",
      "Iteration 15, loss = 0.00103725\n",
      "Iteration 16, loss = 0.00100024\n",
      "Iteration 17, loss = 0.00096606\n",
      "Iteration 18, loss = 0.00093404\n",
      "Iteration 19, loss = 0.00090409\n",
      "Iteration 20, loss = 0.00087550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58157224\n",
      "Iteration 2, loss = 0.01969327\n",
      "Iteration 3, loss = 0.00368646\n",
      "Iteration 4, loss = 0.00237052\n",
      "Iteration 5, loss = 0.00171197\n",
      "Iteration 6, loss = 0.00148876\n",
      "Iteration 7, loss = 0.00136330\n",
      "Iteration 8, loss = 0.00127488\n",
      "Iteration 9, loss = 0.00120711\n",
      "Iteration 10, loss = 0.00115178\n",
      "Iteration 11, loss = 0.00110406\n",
      "Iteration 12, loss = 0.00106157\n",
      "Iteration 13, loss = 0.00102308\n",
      "Iteration 14, loss = 0.00098798\n",
      "Iteration 15, loss = 0.00095502\n",
      "Iteration 16, loss = 0.00092328\n",
      "Iteration 17, loss = 0.00089356\n",
      "Iteration 18, loss = 0.00086528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53783208\n",
      "Iteration 2, loss = 0.01093391\n",
      "Iteration 3, loss = 0.00327811\n",
      "Iteration 4, loss = 0.00238287\n",
      "Iteration 5, loss = 0.00183733\n",
      "Iteration 6, loss = 0.00158678\n",
      "Iteration 7, loss = 0.00142877\n",
      "Iteration 8, loss = 0.00132887\n",
      "Iteration 9, loss = 0.00125336\n",
      "Iteration 10, loss = 0.00119257\n",
      "Iteration 11, loss = 0.00114182\n",
      "Iteration 12, loss = 0.00109741\n",
      "Iteration 13, loss = 0.00105741\n",
      "Iteration 14, loss = 0.00102090\n",
      "Iteration 15, loss = 0.00098709\n",
      "Iteration 16, loss = 0.00095508\n",
      "Iteration 17, loss = 0.00092451\n",
      "Iteration 18, loss = 0.00089588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54578854\n",
      "Iteration 2, loss = 0.01356404\n",
      "Iteration 3, loss = 0.00316475\n",
      "Iteration 4, loss = 0.00232818\n",
      "Iteration 5, loss = 0.00183195\n",
      "Iteration 6, loss = 0.00159320\n",
      "Iteration 7, loss = 0.00144905\n",
      "Iteration 8, loss = 0.00134647\n",
      "Iteration 9, loss = 0.00126962\n",
      "Iteration 10, loss = 0.00120627\n",
      "Iteration 11, loss = 0.00115247\n",
      "Iteration 12, loss = 0.00110593\n",
      "Iteration 13, loss = 0.00106400\n",
      "Iteration 14, loss = 0.00102563\n",
      "Iteration 15, loss = 0.00099015\n",
      "Iteration 16, loss = 0.00095721\n",
      "Iteration 17, loss = 0.00092588\n",
      "Iteration 18, loss = 0.00089619\n",
      "Iteration 19, loss = 0.00086743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54090242\n",
      "Iteration 2, loss = 0.01305925\n",
      "Iteration 3, loss = 0.00315242\n",
      "Iteration 4, loss = 0.00230073\n",
      "Iteration 5, loss = 0.00192376\n",
      "Iteration 6, loss = 0.00171172\n",
      "Iteration 7, loss = 0.00157519\n",
      "Iteration 8, loss = 0.00147315\n",
      "Iteration 9, loss = 0.00139257\n",
      "Iteration 10, loss = 0.00132580\n",
      "Iteration 11, loss = 0.00126802\n",
      "Iteration 12, loss = 0.00121679\n",
      "Iteration 13, loss = 0.00117057\n",
      "Iteration 14, loss = 0.00112797\n",
      "Iteration 15, loss = 0.00108830\n",
      "Iteration 16, loss = 0.00105112\n",
      "Iteration 17, loss = 0.00101574\n",
      "Iteration 18, loss = 0.00098216\n",
      "Iteration 19, loss = 0.00094962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55278272\n",
      "Iteration 2, loss = 0.01229614\n",
      "Iteration 3, loss = 0.00331024\n",
      "Iteration 4, loss = 0.00223984\n",
      "Iteration 5, loss = 0.00181046\n",
      "Iteration 6, loss = 0.00160482\n",
      "Iteration 7, loss = 0.00147616\n",
      "Iteration 8, loss = 0.00137401\n",
      "Iteration 9, loss = 0.00129613\n",
      "Iteration 10, loss = 0.00123345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00118119\n",
      "Iteration 12, loss = 0.00113411\n",
      "Iteration 13, loss = 0.00109081\n",
      "Iteration 14, loss = 0.00105061\n",
      "Iteration 15, loss = 0.00101270\n",
      "Iteration 16, loss = 0.00097706\n",
      "Iteration 17, loss = 0.00094207\n",
      "Iteration 18, loss = 0.00090918\n",
      "Iteration 19, loss = 0.00087761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54023085\n",
      "Iteration 2, loss = 0.01298664\n",
      "Iteration 3, loss = 0.00341987\n",
      "Iteration 4, loss = 0.00231356\n",
      "Iteration 5, loss = 0.00178126\n",
      "Iteration 6, loss = 0.00161206\n",
      "Iteration 7, loss = 0.00149707\n",
      "Iteration 8, loss = 0.00141286\n",
      "Iteration 9, loss = 0.00134321\n",
      "Iteration 10, loss = 0.00128341\n",
      "Iteration 11, loss = 0.00123056\n",
      "Iteration 12, loss = 0.00118285\n",
      "Iteration 13, loss = 0.00113893\n",
      "Iteration 14, loss = 0.00109729\n",
      "Iteration 15, loss = 0.00105826\n",
      "Iteration 16, loss = 0.00102155\n",
      "Iteration 17, loss = 0.00098616\n",
      "Iteration 18, loss = 0.00095210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import xgboost as xgb\n",
    "estimators = []\n",
    "#model1 = LogisticRegression()\n",
    "#estimators.append(('logistic', model1))\n",
    "# model2 = MultinomialNB()\n",
    "# estimators.append(('MulNB', model2))\n",
    "# model2 = DecisionTreeClassifier()\n",
    "# estimators.append(('cart', model2))\n",
    "# model3 = LinearSVC(C = 0.012345444444444445)\n",
    "# estimators.append(('svm', model3))\n",
    "#model2 = LinearSVC(C = 1)\n",
    "#estimators.append(('svm', model2))\n",
    "#model3 = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        #alpha=1e-2, random_state=42, max_iter=5, tol=None)\n",
    "#estimators.append(('SGD', model3))\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "estimators = []\n",
    "model1 = MLPClassifier(hidden_layer_sizes=(10), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN10', model1))\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(11), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN11', model2))\n",
    "model3 = MLPClassifier(hidden_layer_sizes=(14), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN14', model3))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_10_11_14.csv\")\n",
    "\n",
    "estimators = []\n",
    "model4 = MLPClassifier(hidden_layer_sizes=(15), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN15', model4))\n",
    "model5 = MLPClassifier(hidden_layer_sizes=(16), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN16', model5))\n",
    "model6 = MLPClassifier(hidden_layer_sizes=(18), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN18', model6))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_15_16_18.csv\")\n",
    "\n",
    "estimators = []\n",
    "model7 = MLPClassifier(hidden_layer_sizes=(24), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN24', model7))\n",
    "model8 = MLPClassifier(hidden_layer_sizes=(26), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN26', model8))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_24_26.csv\")\n",
    "\n",
    "estimators = []\n",
    "model9 = MLPClassifier(hidden_layer_sizes=(28), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN28', model9))\n",
    "model10 = MLPClassifier(hidden_layer_sizes=(29), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN29', model10))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_28_29.csv\")\n",
    "\n",
    "estimators = []\n",
    "model11 = MLPClassifier(hidden_layer_sizes=(31), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN31', model11))\n",
    "model12 = MLPClassifier(hidden_layer_sizes=(32), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN32', model12))\n",
    "ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_31_32.csv\")\n",
    "\n",
    "\n",
    "model13 = MLPClassifier(hidden_layer_sizes=(33), \n",
    "                       verbose = True,random_state = 0)\n",
    "estimators.append(('NN33', model13))\n",
    "model13.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(model13.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble_33.csv\")\n",
    "\n",
    "# create the ensemble model\n",
    "# ensemble = VotingClassifier(estimators, n_jobs =1)\n",
    "\n",
    "#ensemble.fit(X_train_bigram_numeric_scaled, y_train)\n",
    "#tr = ensemble.score(X_train_bigram_numeric_scaled, y_train)\n",
    "#te = ensemble.score(X_test_bigram_numeric_scaled, y_test)\n",
    "#print(\"train score = {}, test score = {}\\n\".format(tr,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'kaggle_project.ipynb',\n",
       " 'prediction1029.csv',\n",
       " 'prediction1102_v3.csv',\n",
       " 'prediction1103_v1_nn10.csv',\n",
       " 'prediction1103_v2_ensemble.csv',\n",
       " 'prediction1104_v1_ensemble.csv',\n",
       " 'prediction1107_v1.csv',\n",
       " 'prediction1109_v1_nnensemble_10_11_14.csv',\n",
       " 'prediction1109_v1_nnensemble_15_16_18.csv',\n",
       " 'prediction1109_v1_nnensemble_24_26.csv',\n",
       " 'prediction1109_v1_nnensemble_28_29.csv',\n",
       " 'prediction1109_v1_nnensemble_31_32.csv',\n",
       " 'prediction1109_v1_nnensemble_33.csv',\n",
       " 'test.csv',\n",
       " 'train.csv',\n",
       " 'X_test.npz',\n",
       " 'X_train.npz',\n",
       " 'X_train_train.npz',\n",
       " 'X_valid.npz']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir():\n",
    "    r = re.compile(r'prediction1109')\n",
    "    temp = r.search(file)\n",
    "    if temp is not None:\n",
    "        df['{}'.format(file)] = pd.read_csv(file).iloc[:,1]\n",
    "    \n",
    "output = df.apply(lambda x: stats.mode(x)[0][0], axis = 1)\n",
    "\n",
    "output.name = 'Category'\n",
    "output.to_csv('prediction_13NN_ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65204463\n",
      "Iteration 2, loss = 0.03881700\n",
      "Iteration 3, loss = 0.00813349\n",
      "Iteration 4, loss = 0.00473853\n",
      "Iteration 5, loss = 0.00335501\n",
      "Iteration 6, loss = 0.00254591\n",
      "Iteration 7, loss = 0.00207329\n",
      "Iteration 8, loss = 0.00176021\n",
      "Iteration 9, loss = 0.00153931\n",
      "Iteration 10, loss = 0.00137764\n",
      "Iteration 11, loss = 0.00125555\n",
      "Iteration 12, loss = 0.00116023\n",
      "Iteration 13, loss = 0.00108419\n",
      "Iteration 14, loss = 0.00102176\n",
      "Iteration 15, loss = 0.00097010\n",
      "Iteration 16, loss = 0.00092610\n",
      "Iteration 17, loss = 0.00088823\n",
      "Iteration 18, loss = 0.00085523\n",
      "Iteration 19, loss = 0.00082577\n",
      "Iteration 20, loss = 0.00079938\n",
      "Iteration 21, loss = 0.00077540\n",
      "Iteration 22, loss = 0.00075341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70694764\n",
      "Iteration 2, loss = 0.04636713\n",
      "Iteration 3, loss = 0.00950413\n",
      "Iteration 4, loss = 0.00543616\n",
      "Iteration 5, loss = 0.00370199\n",
      "Iteration 6, loss = 0.00279765\n",
      "Iteration 7, loss = 0.00225898\n",
      "Iteration 8, loss = 0.00191420\n",
      "Iteration 9, loss = 0.00167616\n",
      "Iteration 10, loss = 0.00150363\n",
      "Iteration 11, loss = 0.00137459\n",
      "Iteration 12, loss = 0.00127386\n",
      "Iteration 13, loss = 0.00119382\n",
      "Iteration 14, loss = 0.00112844\n",
      "Iteration 15, loss = 0.00107399\n",
      "Iteration 16, loss = 0.00102761\n",
      "Iteration 17, loss = 0.00098739\n",
      "Iteration 18, loss = 0.00095213\n",
      "Iteration 19, loss = 0.00092065\n",
      "Iteration 20, loss = 0.00089237\n",
      "Iteration 21, loss = 0.00086625\n",
      "Iteration 22, loss = 0.00084199\n",
      "Iteration 23, loss = 0.00081943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68375863\n",
      "Iteration 2, loss = 0.04671390\n",
      "Iteration 3, loss = 0.00735352\n",
      "Iteration 4, loss = 0.00393261\n",
      "Iteration 5, loss = 0.00256686\n",
      "Iteration 6, loss = 0.00200934\n",
      "Iteration 7, loss = 0.00163185\n",
      "Iteration 8, loss = 0.00141841\n",
      "Iteration 9, loss = 0.00125611\n",
      "Iteration 10, loss = 0.00113901\n",
      "Iteration 11, loss = 0.00105449\n",
      "Iteration 12, loss = 0.00099155\n",
      "Iteration 13, loss = 0.00094306\n",
      "Iteration 14, loss = 0.00090212\n",
      "Iteration 15, loss = 0.00086764\n",
      "Iteration 16, loss = 0.00083748\n",
      "Iteration 17, loss = 0.00081022\n",
      "Iteration 18, loss = 0.00078586\n",
      "Iteration 19, loss = 0.00076272\n",
      "Iteration 20, loss = 0.00074148\n",
      "Iteration 21, loss = 0.00072133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63453443\n",
      "Iteration 2, loss = 0.03353107\n",
      "Iteration 3, loss = 0.00688116\n",
      "Iteration 4, loss = 0.00310873\n",
      "Iteration 5, loss = 0.00211864\n",
      "Iteration 6, loss = 0.00167863\n",
      "Iteration 7, loss = 0.00146539\n",
      "Iteration 8, loss = 0.00133025\n",
      "Iteration 9, loss = 0.00123435\n",
      "Iteration 10, loss = 0.00116124\n",
      "Iteration 11, loss = 0.00110134\n",
      "Iteration 12, loss = 0.00105095\n",
      "Iteration 13, loss = 0.00100774\n",
      "Iteration 14, loss = 0.00096949\n",
      "Iteration 15, loss = 0.00093468\n",
      "Iteration 16, loss = 0.00090307\n",
      "Iteration 17, loss = 0.00087377\n",
      "Iteration 18, loss = 0.00084641\n",
      "Iteration 19, loss = 0.00082051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58605465\n",
      "Iteration 2, loss = 0.02610711\n",
      "Iteration 3, loss = 0.00540926\n",
      "Iteration 4, loss = 0.00329924\n",
      "Iteration 5, loss = 0.00245302\n",
      "Iteration 6, loss = 0.00196133\n",
      "Iteration 7, loss = 0.00166843\n",
      "Iteration 8, loss = 0.00147666\n",
      "Iteration 9, loss = 0.00133775\n",
      "Iteration 10, loss = 0.00123486\n",
      "Iteration 11, loss = 0.00115445\n",
      "Iteration 12, loss = 0.00108935\n",
      "Iteration 13, loss = 0.00103551\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-149d9a2d80e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_X_train_bigram_numeric_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_X_test_bigram_numeric_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prediction1109_v1_nnensemble.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n\u001b[0;32m    100\u001b[0m                                                  sample_weight=sample_weight)\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'drop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             )\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting.py\u001b[0m in \u001b[0;36m_parallel_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    980\u001b[0m         \"\"\"\n\u001b[0;32m    981\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 982\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_STOCHASTIC_SOLVERS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             self._fit_stochastic(X, y, activations, deltas, coef_grads,\n\u001b[1;32m--> 370\u001b[1;33m                                  intercept_grads, layer_units, incremental)\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;31m# Run the LBFGS solver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_stochastic\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;31m# update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m                     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoef_grads\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\u001b[0m in \u001b[0;36mupdate_params\u001b[1;34m(self, grads)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mSo\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0maligned\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \"\"\"\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mparam\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\u001b[0m in \u001b[0;36m_get_updates\u001b[1;34m(self, grads)\u001b[0m\n\u001b[0;32m    258\u001b[0m                    for m, grad in zip(self.ms, grads)]\n\u001b[0;32m    259\u001b[0m         self.vs = [self.beta_2 * v + (1 - self.beta_2) * (grad ** 2)\n\u001b[1;32m--> 260\u001b[1;33m                    for v, grad in zip(self.vs, grads)]\n\u001b[0m\u001b[0;32m    261\u001b[0m         self.learning_rate = (self.learning_rate_init *\n\u001b[0;32m    262\u001b[0m                               \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m                    for m, grad in zip(self.ms, grads)]\n\u001b[0;32m    259\u001b[0m         self.vs = [self.beta_2 * v + (1 - self.beta_2) * (grad ** 2)\n\u001b[1;32m--> 260\u001b[1;33m                    for v, grad in zip(self.vs, grads)]\n\u001b[0m\u001b[0;32m    261\u001b[0m         self.learning_rate = (self.learning_rate_init *\n\u001b[0;32m    262\u001b[0m                               \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ensemble.fit(real_X_train_bigram_numeric_scaled, Data_train.iloc[:, 6])\n",
    "prediction = pd.DataFrame(ensemble.predict(real_X_test_bigram_numeric_scaled), index = Data_test.test_id)\n",
    "prediction.to_csv(\"prediction1109_v1_nnensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('NN10',\n",
       "                              MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                            batch_size='auto', beta_1=0.9,\n",
       "                                            beta_2=0.999, early_stopping=False,\n",
       "                                            epsilon=1e-08,\n",
       "                                            hidden_layer_sizes=10,\n",
       "                                            learning_rate='constant',\n",
       "                                            learning_rate_init=0.001,\n",
       "                                            max_iter=200, momentum=0.9,\n",
       "                                            n_iter_no_change=10,\n",
       "                                            nesterovs_momentum=True,\n",
       "                                            power_t=0.5, random_state=0,\n",
       "                                            shuffle=True, solver='a...\n",
       "                                            beta_2=0.999, early_stopping=False,\n",
       "                                            epsilon=1e-08,\n",
       "                                            hidden_layer_sizes=33,\n",
       "                                            learning_rate='constant',\n",
       "                                            learning_rate_init=0.001,\n",
       "                                            max_iter=200, momentum=0.9,\n",
       "                                            n_iter_no_change=10,\n",
       "                                            nesterovs_momentum=True,\n",
       "                                            power_t=0.5, random_state=0,\n",
       "                                            shuffle=True, solver='adam',\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=True, warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=3, voting='hard', weights=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
